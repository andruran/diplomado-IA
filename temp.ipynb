{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/husseinlopez/diplomadoIA/blob/main/temp.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M√≥dulo 1: Introducci√≥n a la Miner√≠a de Datos\n",
    "## Ejercicios Pr√°cticos de Limpieza y Preparaci√≥n de Datos\n",
    "\n",
    "**Diplomado en Inteligencia Artificial**  \n",
    "**Dr. Irvin Hussein L√≥pez Nava**  \n",
    "**CICESE - UABC**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos de esta sesi√≥n\n",
    "\n",
    "1. **Identificar y corregir problemas de calidad** en conjuntos de datos reales\n",
    "2. **Manejar valores faltantes** con diferentes estrategias de imputaci√≥n\n",
    "3. **Detectar y tratar valores at√≠picos** sin perder informaci√≥n relevante\n",
    "4. **Aplicar t√©cnicas de reducci√≥n de dimensionalidad** (PCA, t-SNE)\n",
    "5. **Seleccionar atributos relevantes** mediante m√©todos Filter, Wrapper y Embedded\n",
    "6. **Balancear clases desbalanceadas** con t√©cnicas de over/undersampling\n",
    "7. **Integrar todas las t√©cnicas** en un pipeline de preprocesamiento robusto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura del notebook\n",
    "\n",
    "### Parte 1: Limpieza de Datos\n",
    "* Inspecci√≥n inicial y detecci√≥n de problemas\n",
    "* Manejo de valores faltantes\n",
    "* Identificaci√≥n y tratamiento de outliers\n",
    "* Transformaciones y escalamiento\n",
    "\n",
    "### Parte 2: Reducci√≥n de Dimensionalidad\n",
    "* An√°lisis de Componentes Principales (PCA)\n",
    "* t-SNE para visualizaci√≥n no lineal\n",
    "* Comparaci√≥n de m√©todos\n",
    "\n",
    "### Parte 3: Selecci√≥n de Atributos\n",
    "* M√©todos basados en filtros\n",
    "* M√©todos Wrapper\n",
    "* M√©todos Embedded\n",
    "* Consenso entre m√©todos\n",
    "\n",
    "### Parte 4: Balanceo de Clases\n",
    "* T√©cnicas de oversampling (SMOTE, ADASYN)\n",
    "* T√©cnicas de undersampling\n",
    "* Evaluaci√≥n del impacto en m√©tricas\n",
    "\n",
    "### Parte 5: Pipeline Completo\n",
    "* Integraci√≥n de todas las t√©cnicas\n",
    "* Documentaci√≥n de decisiones\n",
    "* Validaci√≥n final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Configuraci√≥n del Entorno\n",
    "\n",
    "Importaremos todas las bibliotecas necesarias para el an√°lisis completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manejo de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Visualizaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuraci√≥n de pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "# Ignorar warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Bibliotecas b√°sicas importadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, RobustScaler,\n",
    "    LabelEncoder, OneHotEncoder, PowerTransformer\n",
    ")\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# Reducci√≥n de dimensionalidad\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Selecci√≥n de atributos\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, chi2, f_classif, mutual_info_classif,\n",
    "    RFE, SelectFromModel\n",
    ")\n",
    "\n",
    "# Modelos para selecci√≥n embedded\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Balanceo de clases\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Evaluaci√≥n y validaci√≥n\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Datasets\n",
    "from sklearn.datasets import (\n",
    "    load_breast_cancer, load_wine, load_iris,\n",
    "    make_classification, make_blobs\n",
    ")\n",
    "\n",
    "print(\"‚úì Bibliotecas de ML y preprocesamiento importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Parte 1: Limpieza de Datos\n",
    "\n",
    "En esta secci√≥n trabajaremos con un dataset que presenta problemas comunes:\n",
    "- Valores faltantes\n",
    "- Valores at√≠picos\n",
    "- Escalas incompatibles\n",
    "- Tipos de datos incorrectos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Creaci√≥n de un Dataset con Problemas Reales\n",
    "\n",
    "Crearemos un dataset sint√©tico que simula datos m√©dicos con problemas t√≠picos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_messy_health_dataset(n_samples=500):\n",
    "    \"\"\"\n",
    "    Crea un dataset sint√©tico de datos de salud con problemas reales:\n",
    "    - Valores faltantes (MCAR, MAR, MNAR)\n",
    "    - Outliers\n",
    "    - Escalas inconsistentes\n",
    "    - Errores de registro\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Variables base\n",
    "    data = {\n",
    "        'edad': np.random.normal(45, 15, n_samples).clip(18, 90),\n",
    "        'peso': np.random.normal(70, 15, n_samples).clip(40, 150),\n",
    "        'estatura': np.random.normal(165, 10, n_samples).clip(140, 200),\n",
    "        'presion_sistolica': np.random.normal(120, 15, n_samples).clip(80, 200),\n",
    "        'presion_diastolica': np.random.normal(80, 10, n_samples).clip(60, 120),\n",
    "        'glucosa': np.random.normal(100, 20, n_samples).clip(70, 300),\n",
    "        'colesterol': np.random.normal(200, 40, n_samples).clip(120, 350),\n",
    "        'trigliceridos': np.random.normal(150, 50, n_samples).clip(50, 500),\n",
    "        'frecuencia_cardiaca': np.random.normal(75, 10, n_samples).clip(50, 120),\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Calcular IMC\n",
    "    df['imc'] = df['peso'] / ((df['estatura']/100) ** 2)\n",
    "    \n",
    "    # Variables categ√≥ricas\n",
    "    df['genero'] = np.random.choice(['M', 'F'], n_samples)\n",
    "    df['fumador'] = np.random.choice(['Si', 'No', 'Exfumador'], n_samples, p=[0.2, 0.6, 0.2])\n",
    "    df['diabetes'] = (df['glucosa'] > 126).astype(int)\n",
    "    df['hipertension'] = (df['presion_sistolica'] > 140).astype(int)\n",
    "    \n",
    "    # Introducir valores faltantes de diferentes tipos\n",
    "    \n",
    "    # MCAR (Missing Completely At Random) - 5% en edad\n",
    "    mcar_mask = np.random.random(n_samples) < 0.05\n",
    "    df.loc[mcar_mask, 'edad'] = np.nan\n",
    "    \n",
    "    # MAR (Missing At Random) - Personas con diabetes tienen m√°s faltantes en colesterol\n",
    "    mar_mask = (df['diabetes'] == 1) & (np.random.random(n_samples) < 0.15)\n",
    "    df.loc[mar_mask, 'colesterol'] = np.nan\n",
    "    \n",
    "    # MNAR (Missing Not At Random) - Valores altos de glucosa tienden a faltar m√°s\n",
    "    high_glucose = df['glucosa'] > df['glucosa'].quantile(0.75)\n",
    "    mnar_mask = high_glucose & (np.random.random(n_samples) < 0.10)\n",
    "    df.loc[mnar_mask, 'glucosa'] = np.nan\n",
    "    \n",
    "    # Valores faltantes adicionales\n",
    "    df.loc[np.random.random(n_samples) < 0.08, 'trigliceridos'] = np.nan\n",
    "    df.loc[np.random.random(n_samples) < 0.03, 'frecuencia_cardiaca'] = np.nan\n",
    "    \n",
    "    # Introducir outliers\n",
    "    \n",
    "    # Outliers extremos (errores de medici√≥n)\n",
    "    outlier_indices = np.random.choice(n_samples, size=10, replace=False)\n",
    "    df.loc[outlier_indices[:3], 'peso'] = np.random.uniform(200, 250, 3)\n",
    "    df.loc[outlier_indices[3:6], 'presion_sistolica'] = np.random.uniform(220, 280, 3)\n",
    "    df.loc[outlier_indices[6:], 'glucosa'] = np.random.uniform(400, 600, 4)\n",
    "    \n",
    "    # Outliers moderados (valores reales pero inusuales)\n",
    "    moderate_outliers = np.random.choice(n_samples, size=20, replace=False)\n",
    "    df.loc[moderate_outliers, 'colesterol'] = np.random.uniform(300, 400, 20)\n",
    "    \n",
    "    # Introducir inconsistencias\n",
    "    \n",
    "    # Algunas estatura en cm, otras (pocas) en metros\n",
    "    error_indices = np.random.choice(n_samples, size=5, replace=False)\n",
    "    df.loc[error_indices, 'estatura'] = df.loc[error_indices, 'estatura'] / 100\n",
    "    \n",
    "    # Calcular variable objetivo (riesgo cardiovascular)\n",
    "    risk_score = (\n",
    "        (df['edad'] > 55).astype(int) * 2 +\n",
    "        (df['imc'] > 30).astype(int) * 2 +\n",
    "        df['diabetes'] * 3 +\n",
    "        df['hipertension'] * 3 +\n",
    "        (df['fumador'] == 'Si').astype(int) * 2 +\n",
    "        (df['colesterol'] > 240).fillna(0).astype(int) * 2\n",
    "    )\n",
    "    \n",
    "    # Binarizar riesgo con algo de ruido\n",
    "    noise = np.random.random(n_samples) < 0.1\n",
    "    df['riesgo_alto'] = ((risk_score >= 6) != noise).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Crear dataset\n",
    "df_health = create_messy_health_dataset(500)\n",
    "\n",
    "print(f\"Dataset creado con {len(df_health)} observaciones y {len(df_health.columns)} variables\")\n",
    "print(f\"\\nPrimeras filas:\")\n",
    "df_health.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Inspecci√≥n Inicial\n",
    "\n",
    "Primer vistazo a la estructura y calidad de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_dataset(df):\n",
    "    \"\"\"\n",
    "    Realiza una inspecci√≥n completa del dataset\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"INSPECCI√ìN GENERAL DEL DATASET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìä Dimensiones: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
    "    print(f\"üíæ Memoria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TIPOS DE DATOS\")\n",
    "    print(\"=\"*80)\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VALORES FALTANTES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = 100 * missing / len(df)\n",
    "    missing_table = pd.DataFrame({\n",
    "        'Columna': missing.index,\n",
    "        'Faltantes': missing.values,\n",
    "        'Porcentaje': missing_pct.values\n",
    "    })\n",
    "    missing_table = missing_table[missing_table['Faltantes'] > 0].sort_values('Porcentaje', ascending=False)\n",
    "    \n",
    "    if len(missing_table) > 0:\n",
    "        print(missing_table.to_string(index=False))\n",
    "        print(f\"\\n‚ö†Ô∏è  Total de valores faltantes: {missing.sum()} ({100*missing.sum()/(df.shape[0]*df.shape[1]):.2f}% del dataset)\")\n",
    "    else:\n",
    "        print(\"‚úì No hay valores faltantes\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ESTAD√çSTICAS DESCRIPTIVAS (VARIABLES NUM√âRICAS)\")\n",
    "    print(\"=\"*80)\n",
    "    print(df.describe().T)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DISTRIBUCI√ìN DE VARIABLES CATEG√ìRICAS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(df[col].value_counts())\n",
    "        print(f\"Valores √∫nicos: {df[col].nunique()}\")\n",
    "\n",
    "inspect_dataset(df_health)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Visualizaci√≥n de Valores Faltantes\n",
    "\n",
    "Entender el patr√≥n de datos faltantes es crucial para decidir c√≥mo manejarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_missing_data(df):\n",
    "    \"\"\"\n",
    "    Crea visualizaciones comprehensivas de valores faltantes\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Matriz de valores faltantes\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    missing_matrix = df.isnull().astype(int)\n",
    "    sns.heatmap(missing_matrix.T, cmap='YlOrRd', cbar=True, ax=ax1,\n",
    "                yticklabels=df.columns, xticklabels=False)\n",
    "    ax1.set_title('Matriz de Valores Faltantes\\n(Amarillo = Presente, Rojo = Faltante)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Observaciones')\n",
    "    \n",
    "    # 2. Porcentaje de valores faltantes por columna\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    missing_pct = 100 * df.isnull().sum() / len(df)\n",
    "    missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=True)\n",
    "    \n",
    "    if len(missing_pct) > 0:\n",
    "        colors = ['#d62728' if x > 10 else '#ff7f0e' if x > 5 else '#2ca02c' for x in missing_pct]\n",
    "        missing_pct.plot(kind='barh', ax=ax2, color=colors)\n",
    "        ax2.set_xlabel('Porcentaje de valores faltantes (%)')\n",
    "        ax2.set_title('Valores Faltantes por Variable', fontweight='bold')\n",
    "        ax2.axvline(x=5, color='orange', linestyle='--', alpha=0.5, label='5%')\n",
    "        ax2.axvline(x=10, color='red', linestyle='--', alpha=0.5, label='10%')\n",
    "        ax2.legend()\n",
    "        ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 3. N√∫mero de valores faltantes por fila\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    missing_per_row = df.isnull().sum(axis=1)\n",
    "    missing_counts = missing_per_row.value_counts().sort_index()\n",
    "    \n",
    "    ax3.bar(missing_counts.index, missing_counts.values, color='steelblue', alpha=0.7)\n",
    "    ax3.set_xlabel('N√∫mero de valores faltantes')\n",
    "    ax3.set_ylabel('N√∫mero de observaciones')\n",
    "    ax3.set_title('Distribuci√≥n de Valores Faltantes por Fila', fontweight='bold')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # A√±adir texto con estad√≠sticas\n",
    "    total_rows_with_missing = (missing_per_row > 0).sum()\n",
    "    ax3.text(0.95, 0.95, \n",
    "             f'Filas con faltantes: {total_rows_with_missing}\\n'\n",
    "             f'Filas completas: {len(df) - total_rows_with_missing}',\n",
    "             transform=ax3.transAxes, fontsize=10,\n",
    "             verticalalignment='top', horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # 4. Correlaci√≥n entre valores faltantes\n",
    "    ax4 = fig.add_subplot(gs[2, :])\n",
    "    missing_corr = df.isnull().corr()\n",
    "    mask = np.triu(np.ones_like(missing_corr), k=1)\n",
    "    \n",
    "    sns.heatmap(missing_corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "                center=0, ax=ax4, cbar_kws={'label': 'Correlaci√≥n'})\n",
    "    ax4.set_title('Correlaci√≥n entre Patrones de Valores Faltantes\\n'\n",
    "                  '(Valores altos sugieren faltantes no aleatorios)', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('An√°lisis Comprehensivo de Valores Faltantes', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig = visualize_missing_data(df_health)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 An√°lisis de Patrones de Valores Faltantes\n",
    "\n",
    "Determinar si los valores faltantes son MCAR, MAR o MNAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis detallado de patrones de valores faltantes\n",
    "def analyze_missing_patterns(df):\n",
    "    \"\"\"\n",
    "    Analiza si los valores faltantes son MCAR, MAR o MNAR\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"AN√ÅLISIS DE PATRONES DE VALORES FALTANTES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Crear indicadores de faltantes\n",
    "    cols_with_missing = df.columns[df.isnull().any()].tolist()\n",
    "    \n",
    "    for col in cols_with_missing:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Variable: {col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        missing_mask = df[col].isnull()\n",
    "        \n",
    "        # Comparar caracter√≠sticas entre observaciones con y sin faltantes\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        numeric_cols = [c for c in numeric_cols if c != col]\n",
    "        \n",
    "        print(\"\\nComparaci√≥n de medias (con faltantes vs sin faltantes):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for other_col in numeric_cols[:5]:  # Limitamos a 5 para no saturar\n",
    "            if df[other_col].notna().sum() > 0:\n",
    "                mean_missing = df.loc[missing_mask, other_col].mean()\n",
    "                mean_present = df.loc[~missing_mask, other_col].mean()\n",
    "                \n",
    "                if pd.notna(mean_missing) and pd.notna(mean_present):\n",
    "                    diff_pct = 100 * (mean_missing - mean_present) / mean_present\n",
    "                    \n",
    "                    # Test t para diferencia de medias\n",
    "                    try:\n",
    "                        t_stat, p_value = stats.ttest_ind(\n",
    "                            df.loc[missing_mask, other_col].dropna(),\n",
    "                            df.loc[~missing_mask, other_col].dropna()\n",
    "                        )\n",
    "                        significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n",
    "                    except:\n",
    "                        p_value = np.nan\n",
    "                        significance = \"\"\n",
    "                    \n",
    "                    print(f\"{other_col:30s}: {mean_present:7.2f} ‚Üí {mean_missing:7.2f} \"\n",
    "                          f\"({diff_pct:+6.1f}%) p={p_value:.3f} {significance}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INTERPRETACI√ìN:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"* = p < 0.05  (diferencia estad√≠sticamente significativa)\")\n",
    "    print(\"** = p < 0.01 (alta significancia)\")\n",
    "    print(\"*** = p < 0.001 (muy alta significancia)\")\n",
    "    print(\"\\nDiferencias significativas sugieren valores faltantes MAR o MNAR\")\n",
    "    print(\"No diferencias sugiere MCAR (Missing Completely At Random)\")\n",
    "\n",
    "analyze_missing_patterns(df_health)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Manejo de Valores Faltantes\n",
    "\n",
    "Compararemos diferentes estrategias de imputaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_imputation_methods(df, column):\n",
    "    \"\"\"\n",
    "    Compara diferentes m√©todos de imputaci√≥n en una columna espec√≠fica\n",
    "    \"\"\"\n",
    "    df_test = df.copy()\n",
    "    missing_mask = df_test[column].isnull()\n",
    "    original_values = df_test.loc[~missing_mask, column].copy()\n",
    "    \n",
    "    methods = {}\n",
    "    \n",
    "    # 1. Eliminaci√≥n\n",
    "    methods['Eliminaci√≥n'] = df_test[column].dropna()\n",
    "    \n",
    "    # 2. Media\n",
    "    imputer_mean = SimpleImputer(strategy='mean')\n",
    "    methods['Media'] = pd.Series(\n",
    "        imputer_mean.fit_transform(df_test[[column]]).ravel(),\n",
    "        index=df_test.index\n",
    "    )\n",
    "    \n",
    "    # 3. Mediana\n",
    "    imputer_median = SimpleImputer(strategy='median')\n",
    "    methods['Mediana'] = pd.Series(\n",
    "        imputer_median.fit_transform(df_test[[column]]).ravel(),\n",
    "        index=df_test.index\n",
    "    )\n",
    "    \n",
    "    # 4. KNN Imputer\n",
    "    numeric_cols = df_test.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if len(numeric_cols) > 1:\n",
    "        imputer_knn = KNNImputer(n_neighbors=5)\n",
    "        df_knn = df_test[numeric_cols].copy()\n",
    "        imputed_knn = imputer_knn.fit_transform(df_knn)\n",
    "        col_idx = numeric_cols.index(column)\n",
    "        methods['KNN (k=5)'] = pd.Series(\n",
    "            imputed_knn[:, col_idx],\n",
    "            index=df_test.index\n",
    "        )\n",
    "    \n",
    "    # Visualizaci√≥n comparativa\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    # Plot original\n",
    "    ax = axes[0]\n",
    "    ax.hist(original_values, bins=30, alpha=0.7, color='gray', edgecolor='black')\n",
    "    ax.axvline(original_values.mean(), color='red', linestyle='--', \n",
    "               linewidth=2, label=f'Media: {original_values.mean():.2f}')\n",
    "    ax.axvline(original_values.median(), color='blue', linestyle='--', \n",
    "               linewidth=2, label=f'Mediana: {original_values.median():.2f}')\n",
    "    ax.set_title('Distribuci√≥n Original\\\\n(sin valores faltantes)', fontweight='bold')\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Plot cada m√©todo\n",
    "    for idx, (method_name, imputed_data) in enumerate(methods.items(), 1):\n",
    "        if idx >= len(axes):\n",
    "            break\n",
    "        ax = axes[idx]\n",
    "        ax.hist(original_values, bins=30, alpha=0.4, color='gray', label='Original', edgecolor='black')\n",
    "        ax.hist(imputed_data.dropna(), bins=30, alpha=0.6, color='steelblue', label=method_name, edgecolor='black')\n",
    "        mean_diff = imputed_data.mean() - original_values.mean()\n",
    "        std_diff = imputed_data.std() - original_values.std()\n",
    "        ax.set_title(f'{method_name}\\\\nŒîmedia: {mean_diff:+.2f}, Œîstd: {std_diff:+.2f}', fontweight='bold')\n",
    "        ax.set_xlabel(column)\n",
    "        ax.set_ylabel('Frecuencia')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    for idx in range(len(methods) + 1, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Comparaci√≥n de M√©todos de Imputaci√≥n: {column}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Estad√≠sticas\n",
    "    print(\"=\"*80)\n",
    "    print(f\"COMPARACI√ìN DE M√âTODOS DE IMPUTACI√ìN: {column}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\\\nOriginal: N={len(original_values)}, Media={original_values.mean():.2f}, Std={original_values.std():.2f}\")\n",
    "    for method_name, imputed_data in methods.items():\n",
    "        print(f\"{method_name}: N={len(imputed_data.dropna())}, Media={imputed_data.mean():.2f}, Std={imputed_data.std():.2f}\")\n",
    "    \n",
    "    return fig, methods\n",
    "\n",
    "# Comparar m√©todos para glucosa\n",
    "fig, methods = compare_imputation_methods(df_health, 'glucosa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar imputaci√≥n con KNN\n",
    "def apply_imputation(df, strategy='knn'):\n",
    "    \"\"\"\n",
    "    Aplica estrategia de imputaci√≥n al dataset completo\n",
    "    \"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df_imputed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    if strategy == 'knn':\n",
    "        imputer_num = KNNImputer(n_neighbors=5)\n",
    "        df_imputed[numeric_cols] = imputer_num.fit_transform(df_imputed[numeric_cols])\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if df_imputed[col].isnull().any():\n",
    "                mode_value = df_imputed[col].mode()[0]\n",
    "                df_imputed[col].fillna(mode_value, inplace=True)\n",
    "    \n",
    "    print(f\"Imputaci√≥n aplicada con estrategia: {strategy}\")\n",
    "    print(f\"Filas antes: {len(df)} ‚Üí Filas despu√©s: {len(df_imputed)}\")\n",
    "    print(f\"Valores faltantes restantes: {df_imputed.isnull().sum().sum()}\")\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "df_health_imputed = apply_imputation(df_health, strategy='knn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Detecci√≥n y Tratamiento de Outliers\n",
    "\n",
    "Identificaremos valores at√≠picos usando m√∫ltiples m√©todos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_multiple_methods(df, column):\n",
    "    \"\"\"\n",
    "    Detecta outliers usando diferentes m√©todos:\n",
    "    1. IQR (Interquartile Range)\n",
    "    2. Z-score\n",
    "    3. Isolation Forest\n",
    "    \"\"\"\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    \n",
    "    data = df[column].dropna().values.reshape(-1, 1)\n",
    "    outliers = {}\n",
    "    \n",
    "    # 1. M√©todo IQR\n",
    "    Q1 = np.percentile(data, 25)\n",
    "    Q3 = np.percentile(data, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers['IQR'] = (data < lower_bound) | (data > upper_bound)\n",
    "    \n",
    "    # 2. Z-score\n",
    "    z_scores = np.abs(stats.zscore(data))\n",
    "    outliers['Z-score'] = z_scores > 3\n",
    "    \n",
    "    # 3. Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    outliers['Isolation Forest'] = iso_forest.fit_predict(data) == -1\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Box plot\n",
    "    ax = axes[0, 0]\n",
    "    bp = ax.boxplot([data.ravel()], vert=True, patch_artist=True,\n",
    "                     boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                     medianprops=dict(color='red', linewidth=2))\n",
    "    ax.axhline(lower_bound, color='orange', linestyle='--', label=f'IQR lower: {lower_bound:.2f}')\n",
    "    ax.axhline(upper_bound, color='orange', linestyle='--', label=f'IQR upper: {upper_bound:.2f}')\n",
    "    ax.set_ylabel(column)\n",
    "    ax.set_title('Box Plot con L√≠mites IQR', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Distribuci√≥n con outliers\n",
    "    ax = axes[0, 1]\n",
    "    ax.hist(data, bins=50, alpha=0.6, color='steelblue', edgecolor='black')\n",
    "    for method_name, is_outlier in outliers.items():\n",
    "        outlier_values = data[is_outlier.ravel()]\n",
    "        if len(outlier_values) > 0:\n",
    "            ax.scatter(outlier_values, [0] * len(outlier_values), s=100, alpha=0.6, label=method_name)\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    ax.set_title('Distribuci√≥n con Outliers Detectados', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Z-scores\n",
    "    ax = axes[1, 0]\n",
    "    sorted_idx = np.argsort(data.ravel())\n",
    "    ax.scatter(range(len(data)), z_scores[sorted_idx], alpha=0.5, s=20)\n",
    "    ax.axhline(3, color='red', linestyle='--', label='Umbral Z=3')\n",
    "    ax.set_xlabel('Observaciones (ordenadas)')\n",
    "    ax.set_ylabel('|Z-score|')\n",
    "    ax.set_title('Z-scores', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Comparaci√≥n\n",
    "    ax = axes[1, 1]\n",
    "    method_names = list(outliers.keys())\n",
    "    counts = [outliers[m].sum() for m in method_names]\n",
    "    bars = ax.barh(method_names, counts, color=['#ff7f0e', '#2ca02c', '#d62728'])\n",
    "    ax.set_xlabel('N√∫mero de outliers detectados')\n",
    "    ax.set_title('Comparaci√≥n de M√©todos', fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for bar, count in zip(bars, counts):\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2,\n",
    "               f'{int(count)} ({100*count/len(data):.1f}%)',\n",
    "               ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle(f'Detecci√≥n de Outliers: {column}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Consenso\n",
    "    consensus_outliers = sum(outliers.values()) >= 2\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"DETECCI√ìN DE OUTLIERS: {column}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\\\nTotal observaciones: {len(data)}\")\n",
    "    for method, is_outlier in outliers.items():\n",
    "        n_outliers = is_outlier.sum()\n",
    "        print(f\"{method:20s}: {n_outliers:4d} ({100*n_outliers/len(data):5.2f}%)\")\n",
    "    print(f\"\\\\nConsenso (‚â•2 m√©todos): {consensus_outliers.sum()} ({100*consensus_outliers.sum()/len(data):.2f}%)\")\n",
    "    \n",
    "    return fig, outliers, consensus_outliers\n",
    "\n",
    "# Detectar outliers\n",
    "fig_out, outliers_peso, consensus_peso = detect_outliers_multiple_methods(df_health_imputed, 'peso')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de outliers\n",
    "def treat_outliers(df, column, method='cap', outlier_mask=None):\n",
    "    \"\"\"\n",
    "    Trata outliers usando diferentes estrategias\n",
    "    \"\"\"\n",
    "    df_treated = df.copy()\n",
    "    original = df_treated[column].copy()\n",
    "    \n",
    "    if outlier_mask is None:\n",
    "        Q1 = df_treated[column].quantile(0.25)\n",
    "        Q3 = df_treated[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_mask = (df_treated[column] < Q1 - 1.5*IQR) | (df_treated[column] > Q3 + 1.5*IQR)\n",
    "    \n",
    "    if method == 'remove':\n",
    "        df_treated = df_treated[~outlier_mask]\n",
    "    elif method == 'cap':\n",
    "        lower = df_treated[column].quantile(0.05)\n",
    "        upper = df_treated[column].quantile(0.95)\n",
    "        df_treated[column] = df_treated[column].clip(lower, upper)\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    axes[0].hist(original, bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "    axes[0].set_title('Antes del Tratamiento', fontweight='bold')\n",
    "    axes[0].set_xlabel(column)\n",
    "    \n",
    "    axes[1].hist(df_treated[column], bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[1].set_title(f'Despu√©s ({method})', fontweight='bold')\n",
    "    axes[1].set_xlabel(column)\n",
    "    \n",
    "    axes[2].boxplot([original.dropna(), df_treated[column].dropna()],\n",
    "                    labels=['Antes', 'Despu√©s'], patch_artist=True)\n",
    "    axes[2].set_title('Comparaci√≥n', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return df_treated, fig\n",
    "\n",
    "df_peso_treated, fig_treat = treat_outliers(df_health_imputed, 'peso', method='cap', outlier_mask=consensus_peso.ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Escalamiento y Transformaciones\n",
    "\n",
    "Comparaci√≥n de diferentes m√©todos de escalamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_scaling_methods(df, columns=None):\n",
    "    \"\"\"\n",
    "    Compara diferentes m√©todos de escalamiento\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns[:4]\n",
    "    \n",
    "    df_subset = df[columns].copy()\n",
    "    \n",
    "    scalers = {\n",
    "        'Original': None,\n",
    "        'StandardScaler': StandardScaler(),\n",
    "        'MinMaxScaler': MinMaxScaler(),\n",
    "        'RobustScaler': RobustScaler(),\n",
    "        'PowerTransformer': PowerTransformer(method='yeo-johnson')\n",
    "    }\n",
    "    \n",
    "    scaled_data = {}\n",
    "    for name, scaler in scalers.items():\n",
    "        if scaler is None:\n",
    "            scaled_data[name] = df_subset.values\n",
    "        else:\n",
    "            scaled_data[name] = scaler.fit_transform(df_subset)\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, axes = plt.subplots(len(scalers), len(columns), figsize=(5*len(columns), 4*len(scalers)))\n",
    "    if len(columns) == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i, (method_name, data) in enumerate(scaled_data.items()):\n",
    "        for j, col in enumerate(columns):\n",
    "            ax = axes[i, j]\n",
    "            ax.hist(data[:, j], bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "            mean = np.mean(data[:, j])\n",
    "            std = np.std(data[:, j])\n",
    "            if i == 0:\n",
    "                ax.set_title(f'{col}\\\\n{method_name}\\\\nŒº={mean:.2f}, œÉ={std:.2f}', fontweight='bold')\n",
    "            else:\n",
    "                ax.set_title(f'{method_name}\\\\nŒº={mean:.2f}, œÉ={std:.2f}', fontweight='bold')\n",
    "            ax.axvline(mean, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "            ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Comparaci√≥n de M√©todos de Escalamiento', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    return fig, scaled_data\n",
    "\n",
    "cols_to_scale = ['edad', 'peso', 'presion_sistolica', 'glucosa']\n",
    "fig_scale, scaled_results = compare_scaling_methods(df_health_imputed, cols_to_scale)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Parte 2: Reducci√≥n de Dimensionalidad\n",
    "\n",
    "Exploraremos t√©cnicas para reducir el n√∫mero de variables preservando la mayor cantidad de informaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preparaci√≥n: Dataset de C√°ncer de Mama\n",
    "\n",
    "Usaremos el dataset cl√°sico de Wisconsin Breast Cancer con 30 caracter√≠sticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "y_cancer = cancer.target\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET: Wisconsin Breast Cancer\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDimensiones: {X_cancer.shape}\")\n",
    "print(f\"Clases: {np.unique(y_cancer, return_counts=True)}\")\n",
    "print(f\"\\nPrimeras caracter√≠sticas:\")\n",
    "print(X_cancer.columns.tolist()[:10])\n",
    "print(\"...\")\n",
    "\n",
    "# Escalamiento previo (necesario para PCA y t-SNE)\n",
    "scaler = StandardScaler()\n",
    "X_cancer_scaled = scaler.fit_transform(X_cancer)\n",
    "X_cancer_scaled_df = pd.DataFrame(X_cancer_scaled, columns=X_cancer.columns)\n",
    "\n",
    "print(f\"\\n‚úì Datos escalados con StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 An√°lisis de Componentes Principales (PCA)\n",
    "\n",
    "PCA encuentra direcciones ortogonales de m√°xima varianza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pca_analysis(X, y=None, feature_names=None):\n",
    "    \"\"\"\n",
    "    Realiza an√°lisis completo de PCA con m√∫ltiples visualizaciones\n",
    "    \"\"\"\n",
    "    # PCA completo\n",
    "    pca_full = PCA()\n",
    "    X_pca_full = pca_full.fit_transform(X)\n",
    "    \n",
    "    explained_variance = pca_full.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    \n",
    "    # Encontrar componentes para 90%, 95%, 99%\n",
    "    n_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "    n_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "    n_99 = np.argmax(cumulative_variance >= 0.99) + 1\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"AN√ÅLISIS PCA\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nDimensiones originales: {X.shape[1]}\")\n",
    "    print(f\"\\nComponentes necesarios para:\")\n",
    "    print(f\"  - 90% varianza: {n_90} componentes\")\n",
    "    print(f\"  - 95% varianza: {n_95} componentes\")\n",
    "    print(f\"  - 99% varianza: {n_99} componentes\")\n",
    "    print(f\"\\nPrimeros 5 componentes explican: {cumulative_variance[4]:.1%}\")\n",
    "    print(f\"Primeros 10 componentes explican: {cumulative_variance[9]:.1%}\")\n",
    "    \n",
    "    # Visualizaciones\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Varianza por componente (Scree Plot)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    components = np.arange(1, min(21, len(explained_variance)+1))\n",
    "    ax1.bar(components, explained_variance[:20], alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    ax1.set_xlabel('Componente Principal', fontsize=12)\n",
    "    ax1.set_ylabel('Varianza Explicada', fontsize=12)\n",
    "    ax1.set_title('Scree Plot\\n(Primeras 20 componentes)', fontweight='bold', fontsize=13)\n",
    "    ax1.grid(alpha=0.3, axis='y')\n",
    "    ax1.set_xticks(components[::2])\n",
    "    \n",
    "    # 2. Varianza acumulada\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(range(1, len(cumulative_variance)+1), cumulative_variance, \n",
    "             marker='o', linewidth=2, markersize=4, color='steelblue')\n",
    "    ax2.axhline(y=0.90, color='green', linestyle='--', linewidth=2, label='90%', alpha=0.7)\n",
    "    ax2.axhline(y=0.95, color='orange', linestyle='--', linewidth=2, label='95%', alpha=0.7)\n",
    "    ax2.axhline(y=0.99, color='red', linestyle='--', linewidth=2, label='99%', alpha=0.7)\n",
    "    ax2.axvline(x=n_95, color='orange', linestyle=':', alpha=0.5)\n",
    "    ax2.set_xlabel('N√∫mero de Componentes', fontsize=12)\n",
    "    ax2.set_ylabel('Varianza Acumulada', fontsize=12)\n",
    "    ax2.set_title('Varianza Explicada Acumulada', fontweight='bold', fontsize=13)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(alpha=0.3)\n",
    "    ax2.set_xlim(0, min(30, len(cumulative_variance)))\n",
    "    \n",
    "    # 3. Raz√≥n de varianza (Kaiser criterion)\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    eigenvalues = pca_full.explained_variance_[:20]\n",
    "    ax3.plot(range(1, len(eigenvalues)+1), eigenvalues, marker='s', \n",
    "             linewidth=2, markersize=6, color='darkred')\n",
    "    ax3.axhline(y=1, color='black', linestyle='--', linewidth=2, label='Kaiser criterion (Œª=1)', alpha=0.7)\n",
    "    ax3.set_xlabel('Componente Principal', fontsize=12)\n",
    "    ax3.set_ylabel('Eigenvalue (Œª)', fontsize=12)\n",
    "    ax3.set_title('Eigenvalues\\n(Kaiser: retener Œª > 1)', fontweight='bold', fontsize=13)\n",
    "    ax3.legend(fontsize=10)\n",
    "    ax3.grid(alpha=0.3)\n",
    "    n_kaiser = np.sum(pca_full.explained_variance_ > 1)\n",
    "    ax3.text(0.98, 0.98, f'n={n_kaiser}', transform=ax3.transAxes,\n",
    "             ha='right', va='top', fontsize=11, fontweight='bold',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # 4. Proyecci√≥n 2D (PC1 vs PC2)\n",
    "    ax4 = fig.add_subplot(gs[1, :2])\n",
    "    if y is not None:\n",
    "        scatter = ax4.scatter(X_pca_full[:, 0], X_pca_full[:, 1], \n",
    "                            c=y, cmap='RdYlGn', alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "        cbar = plt.colorbar(scatter, ax=ax4)\n",
    "        cbar.set_label('Clase', fontsize=11)\n",
    "    else:\n",
    "        ax4.scatter(X_pca_full[:, 0], X_pca_full[:, 1], \n",
    "                   alpha=0.6, s=50, color='steelblue', edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax4.set_xlabel(f'PC1 ({explained_variance[0]:.1%} varianza)', fontsize=12)\n",
    "    ax4.set_ylabel(f'PC2 ({explained_variance[1]:.1%} varianza)', fontsize=12)\n",
    "    ax4.set_title(f'Proyecci√≥n en Primeras 2 Componentes\\n(Total: {explained_variance[0]+explained_variance[1]:.1%} varianza)', \n",
    "                 fontweight='bold', fontsize=13)\n",
    "    ax4.grid(alpha=0.3)\n",
    "    ax4.axhline(0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    ax4.axvline(0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # 5. Loadings PC1\n",
    "    ax5 = fig.add_subplot(gs[1, 2])\n",
    "    if feature_names is not None:\n",
    "        loadings_pc1 = pd.Series(pca_full.components_[0], index=feature_names)\n",
    "        top_loadings = pd.concat([loadings_pc1.nlargest(5), loadings_pc1.nsmallest(5)])\n",
    "        colors = ['red' if x < 0 else 'green' for x in top_loadings.values]\n",
    "        top_loadings.plot(kind='barh', ax=ax5, color=colors, alpha=0.7, edgecolor='black')\n",
    "        ax5.set_xlabel('Loading', fontsize=11)\n",
    "        ax5.set_title('Top Loadings PC1', fontweight='bold', fontsize=13)\n",
    "        ax5.axvline(0, color='black', linewidth=1)\n",
    "        ax5.grid(alpha=0.3, axis='x')\n",
    "    \n",
    "    # 6. Loadings PC2\n",
    "    ax6 = fig.add_subplot(gs[2, 0])\n",
    "    if feature_names is not None:\n",
    "        loadings_pc2 = pd.Series(pca_full.components_[1], index=feature_names)\n",
    "        top_loadings = pd.concat([loadings_pc2.nlargest(5), loadings_pc2.nsmallest(5)])\n",
    "        colors = ['red' if x < 0 else 'green' for x in top_loadings.values]\n",
    "        top_loadings.plot(kind='barh', ax=ax6, color=colors, alpha=0.7, edgecolor='black')\n",
    "        ax6.set_xlabel('Loading', fontsize=11)\n",
    "        ax6.set_title('Top Loadings PC2', fontweight='bold', fontsize=13)\n",
    "        ax6.axvline(0, color='black', linewidth=1)\n",
    "        ax6.grid(alpha=0.3, axis='x')\n",
    "    \n",
    "    # 7. Biplot (PC1 vs PC2 con vectores)\n",
    "    ax7 = fig.add_subplot(gs[2, 1:])\n",
    "    if y is not None:\n",
    "        scatter = ax7.scatter(X_pca_full[:, 0], X_pca_full[:, 1], \n",
    "                            c=y, cmap='RdYlGn', alpha=0.3, s=30)\n",
    "    else:\n",
    "        ax7.scatter(X_pca_full[:, 0], X_pca_full[:, 1], alpha=0.3, s=30, color='gray')\n",
    "    \n",
    "    if feature_names is not None:\n",
    "        # Dibujar vectores de variables (solo las m√°s importantes)\n",
    "        scale = 4\n",
    "        top_features = np.argsort(np.abs(pca_full.components_[0]))[-8:]\n",
    "        for i in top_features:\n",
    "            ax7.arrow(0, 0, \n",
    "                     pca_full.components_[0, i]*scale, \n",
    "                     pca_full.components_[1, i]*scale,\n",
    "                     head_width=0.1, head_length=0.1, fc='red', ec='red', alpha=0.6, linewidth=2)\n",
    "            ax7.text(pca_full.components_[0, i]*scale*1.15, \n",
    "                    pca_full.components_[1, i]*scale*1.15,\n",
    "                    feature_names[i], fontsize=9, ha='center', \n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "    \n",
    "    ax7.set_xlabel(f'PC1 ({explained_variance[0]:.1%})', fontsize=12)\n",
    "    ax7.set_ylabel(f'PC2 ({explained_variance[1]:.1%})', fontsize=12)\n",
    "    ax7.set_title('Biplot (Observaciones + Variables)', fontweight='bold', fontsize=13)\n",
    "    ax7.axhline(0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    ax7.axvline(0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    ax7.grid(alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('An√°lisis Completo de Componentes Principales (PCA)', \n",
    "                fontsize=18, fontweight='bold', y=0.998)\n",
    "    \n",
    "    return pca_full, X_pca_full, fig\n",
    "\n",
    "# Aplicar PCA\n",
    "pca_model, X_cancer_pca, fig_pca = perform_pca_analysis(\n",
    "    X_cancer_scaled, \n",
    "    y_cancer, \n",
    "    X_cancer.columns\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Visualizaci√≥n 3D con PCA\n",
    "\n",
    "Exploremos las primeras 3 componentes en un gr√°fico interactivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_3d_interactive(X_pca, y=None, explained_variance=None):\n",
    "    \"\"\"\n",
    "    Crea visualizaci√≥n 3D interactiva de PCA\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    if y is not None:\n",
    "        # Colores por clase\n",
    "        colors = ['red' if label == 0 else 'green' for label in y]\n",
    "        labels = ['Maligno' if label == 0 else 'Benigno' for label in y]\n",
    "        \n",
    "        for class_label in np.unique(y):\n",
    "            mask = y == class_label\n",
    "            class_name = 'Maligno' if class_label == 0 else 'Benigno'\n",
    "            color = 'red' if class_label == 0 else 'green'\n",
    "            \n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=X_pca[mask, 0],\n",
    "                y=X_pca[mask, 1],\n",
    "                z=X_pca[mask, 2],\n",
    "                mode='markers',\n",
    "                name=class_name,\n",
    "                marker=dict(\n",
    "                    size=5,\n",
    "                    color=color,\n",
    "                    opacity=0.6,\n",
    "                    line=dict(color='black', width=0.5)\n",
    "                ),\n",
    "                text=[class_name] * mask.sum(),\n",
    "                hovertemplate='<b>%{text}</b><br>PC1: %{x:.2f}<br>PC2: %{y:.2f}<br>PC3: %{z:.2f}<extra></extra>'\n",
    "            ))\n",
    "    else:\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=X_pca[:, 0],\n",
    "            y=X_pca[:, 1],\n",
    "            z=X_pca[:, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(size=5, color='steelblue', opacity=0.6),\n",
    "        ))\n",
    "    \n",
    "    # Etiquetas de ejes\n",
    "    if explained_variance is not None:\n",
    "        xlabel = f'PC1 ({explained_variance[0]:.1%})'\n",
    "        ylabel = f'PC2 ({explained_variance[1]:.1%})'\n",
    "        zlabel = f'PC3 ({explained_variance[2]:.1%})'\n",
    "        total_var = explained_variance[0] + explained_variance[1] + explained_variance[2]\n",
    "        title = f'PCA 3D - Varianza Total: {total_var:.1%}'\n",
    "    else:\n",
    "        xlabel, ylabel, zlabel = 'PC1', 'PC2', 'PC3'\n",
    "        title = 'PCA 3D'\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(text=title, font=dict(size=20, color='black'), x=0.5, xanchor='center'),\n",
    "        scene=dict(\n",
    "            xaxis=dict(title=xlabel, backgroundcolor='rgb(230, 230,230)'),\n",
    "            yaxis=dict(title=ylabel, backgroundcolor='rgb(230, 230,230)'),\n",
    "            zaxis=dict(title=zlabel, backgroundcolor='rgb(230, 230,230)'),\n",
    "        ),\n",
    "        width=900,\n",
    "        height=700,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Crear visualizaci√≥n 3D\n",
    "fig_3d = plot_pca_3d_interactive(\n",
    "    X_cancer_pca, \n",
    "    y_cancer, \n",
    "    pca_model.explained_variance_ratio_\n",
    ")\n",
    "fig_3d.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 t-SNE para Visualizaci√≥n No Lineal\n",
    "\n",
    "t-SNE (t-Distributed Stochastic Neighbor Embedding) preserva la estructura local de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_perplexity_tsne(X, y=None, perplexities=[10, 30, 50, 90]):\n",
    "    \"\"\"\n",
    "    Compara t-SNE con diferentes valores de perplexity\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"Ejecutando t-SNE con diferentes perplexities...\")\n",
    "    print(\"(Esto puede tomar varios minutos)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for perp in perplexities:\n",
    "        print(f\"  Perplexity = {perp}...\", end=' ')\n",
    "        tsne = TSNE(n_components=2, perplexity=perp, random_state=42, \n",
    "                   n_iter=1000, verbose=0)\n",
    "        X_tsne = tsne.fit_transform(X)\n",
    "        results[perp] = X_tsne\n",
    "        print(\"‚úì\")\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, perp in enumerate(perplexities):\n",
    "        ax = axes[idx]\n",
    "        X_tsne = results[perp]\n",
    "        \n",
    "        if y is not None:\n",
    "            scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                               c=y, cmap='RdYlGn', alpha=0.6, s=50, \n",
    "                               edgecolors='black', linewidth=0.5)\n",
    "            if idx == 0:\n",
    "                cbar = plt.colorbar(scatter, ax=ax)\n",
    "                cbar.set_label('Clase (0=Maligno, 1=Benigno)', fontsize=10)\n",
    "        else:\n",
    "            ax.scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                      alpha=0.6, s=50, color='steelblue',\n",
    "                      edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        ax.set_xlabel('t-SNE 1', fontsize=12)\n",
    "        ax.set_ylabel('t-SNE 2', fontsize=12)\n",
    "        ax.set_title(f'Perplexity = {perp}', fontweight='bold', fontsize=14)\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Comparaci√≥n de t-SNE con Diferentes Perplexities', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INTERPRETACI√ìN DE PERPLEXITY:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"  ‚Ä¢ Valores bajos (5-15): Enfatizan estructura LOCAL\")\n",
    "    print(\"  ‚Ä¢ Valores medios (30-50): Balance entre local y global\")\n",
    "    print(\"  ‚Ä¢ Valores altos (>50): Enfatizan estructura GLOBAL\")\n",
    "    print(\"  ‚Ä¢ Valor por defecto: 30 (buen punto de partida)\")\n",
    "    \n",
    "    return results, fig\n",
    "\n",
    "# Ejecutar t-SNE\n",
    "# NOTA: Usar subset para velocidad (t-SNE es costoso)\n",
    "tsne_results, fig_tsne = compare_perplexity_tsne(\n",
    "    X_cancer_scaled[:500], \n",
    "    y_cancer[:500],\n",
    "    perplexities=[10, 30, 50, 90]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Comparaci√≥n: PCA vs t-SNE\n",
    "\n",
    "Ventajas y desventajas de cada m√©todo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pca_tsne(X, y=None, n_components_pca=2):\n",
    "    \"\"\"\n",
    "    Compara lado a lado PCA y t-SNE\n",
    "    \"\"\"\n",
    "    # PCA\n",
    "    pca = PCA(n_components=n_components_pca)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    # t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    \n",
    "    # PCA\n",
    "    ax = axes[0]\n",
    "    if y is not None:\n",
    "        scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                           c=y, cmap='RdYlGn', alpha=0.6, s=60,\n",
    "                           edgecolors='black', linewidth=0.5)\n",
    "        plt.colorbar(scatter, ax=ax, label='Clase')\n",
    "    else:\n",
    "        ax.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6, s=60, color='steelblue')\n",
    "    \n",
    "    var_exp = pca.explained_variance_ratio_\n",
    "    ax.set_xlabel(f'PC1 ({var_exp[0]:.1%})', fontsize=13)\n",
    "    ax.set_ylabel(f'PC2 ({var_exp[1]:.1%})', fontsize=13)\n",
    "    ax.set_title(f'PCA\\nVarianza total: {var_exp.sum():.1%}', \n",
    "                fontweight='bold', fontsize=15)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.axhline(0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    ax.axvline(0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # A√±adir caracter√≠sticas de PCA\n",
    "    ax.text(0.02, 0.98, \n",
    "           '‚úì Lineal\\n‚úì R√°pido\\n‚úì Interpretable\\n‚úì Determin√≠stico\\n‚úó Asume linealidad',\n",
    "           transform=ax.transAxes, fontsize=11, verticalalignment='top',\n",
    "           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # t-SNE\n",
    "    ax = axes[1]\n",
    "    if y is not None:\n",
    "        scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                           c=y, cmap='RdYlGn', alpha=0.6, s=60,\n",
    "                           edgecolors='black', linewidth=0.5)\n",
    "        plt.colorbar(scatter, ax=ax, label='Clase')\n",
    "    else:\n",
    "        ax.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.6, s=60, color='steelblue')\n",
    "    \n",
    "    ax.set_xlabel('t-SNE 1', fontsize=13)\n",
    "    ax.set_ylabel('t-SNE 2', fontsize=13)\n",
    "    ax.set_title('t-SNE\\n(perplexity=30)', fontweight='bold', fontsize=15)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # A√±adir caracter√≠sticas de t-SNE\n",
    "    ax.text(0.02, 0.98,\n",
    "           '‚úì No lineal\\n‚úì Preserva clusters\\n‚úì Bueno para visualizaci√≥n\\n‚úó Lento\\n‚úó No determin√≠stico',\n",
    "           transform=ax.transAxes, fontsize=11, verticalalignment='top',\n",
    "           bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.suptitle('Comparaci√≥n: PCA vs t-SNE', fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return X_pca, X_tsne, fig\n",
    "\n",
    "# Comparar\n",
    "X_pca_2d, X_tsne_2d, fig_compare = compare_pca_tsne(X_cancer_scaled, y_cancer)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CU√ÅNDO USAR CADA M√âTODO\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nPCA:\")\n",
    "print(\"  ‚Ä¢ Reducci√≥n de dimensionalidad para modelado\")\n",
    "print(\"  ‚Ä¢ Interpretabilidad importante\")\n",
    "print(\"  ‚Ä¢ Datasets grandes\")\n",
    "print(\"  ‚Ä¢ Necesitas reproducibilidad\")\n",
    "print(\"\\nt-SNE:\")\n",
    "print(\"  ‚Ä¢ Visualizaci√≥n exploratoria\")\n",
    "print(\"  ‚Ä¢ Detectar clusters no lineales\")\n",
    "print(\"  ‚Ä¢ Datasets peque√±os-medianos (<10k observaciones)\")\n",
    "print(\"  ‚Ä¢ No necesitas interpretabilidad de ejes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Parte 3: Selecci√≥n de Atributos\n",
    "\n",
    "Identificaremos las caracter√≠sticas m√°s relevantes usando tres familias de m√©todos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 M√©todos Filter\n",
    "\n",
    "Eval√∫an la relevancia de cada atributo independientemente del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filter_methods(X, y, k=15):\n",
    "    \"\"\"\n",
    "    Aplica m√∫ltiples m√©todos filter para selecci√≥n de atributos\n",
    "    \"\"\"\n",
    "    feature_names = X.columns if hasattr(X, 'columns') else [f'F{i}' for i in range(X.shape[1])]\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"M√âTODOS FILTER - SELECCI√ìN DE ATRIBUTOS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. ANOVA F-test (para clasificaci√≥n)\n",
    "    print(\"\\n1. Ejecutando ANOVA F-test...\", end=' ')\n",
    "    f_selector = SelectKBest(f_classif, k='all')\n",
    "    f_selector.fit(X, y)\n",
    "    results['F-test'] = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Score': f_selector.scores_,\n",
    "        'p-value': f_selector.pvalues_\n",
    "    }).sort_values('Score', ascending=False)\n",
    "    print(\"‚úì\")\n",
    "    \n",
    "    # 2. Mutual Information\n",
    "    print(\"2. Ejecutando Mutual Information...\", end=' ')\n",
    "    mi_selector = SelectKBest(mutual_info_classif, k='all')\n",
    "    mi_selector.fit(X, y)\n",
    "    results['Mutual Info'] = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Score': mi_selector.scores_\n",
    "    }).sort_values('Score', ascending=False)\n",
    "    print(\"‚úì\")\n",
    "    \n",
    "    # 3. Chi-squared (requiere valores no negativos)\n",
    "    print(\"3. Ejecutando Chi-squared...\", end=' ')\n",
    "    # Normalizar a [0, 1] para chi2\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    X_normalized = scaler.fit_transform(X)\n",
    "    chi2_selector = SelectKBest(chi2, k='all')\n",
    "    chi2_selector.fit(X_normalized, y)\n",
    "    results['Chi-squared'] = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Score': chi2_selector.scores_\n",
    "    }).sort_values('Score', ascending=False)\n",
    "    print(\"‚úì\")\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig = plt.figure(figsize=(20, 6))\n",
    "    \n",
    "    for idx, (method_name, scores_df) in enumerate(results.items(), 1):\n",
    "        ax = plt.subplot(1, 3, idx)\n",
    "        top_features = scores_df.head(k)\n",
    "        \n",
    "        # Colores basados en score normalizado\n",
    "        scores_norm = (top_features['Score'] - top_features['Score'].min()) / (top_features['Score'].max() - top_features['Score'].min())\n",
    "        colors = plt.cm.RdYlGn(scores_norm)\n",
    "        \n",
    "        bars = ax.barh(range(len(top_features)), top_features['Score'].values, color=colors, edgecolor='black')\n",
    "        ax.set_yticks(range(len(top_features)))\n",
    "        ax.set_yticklabels(top_features['Feature'].values, fontsize=10)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_xlabel('Score', fontsize=12)\n",
    "        ax.set_title(f'{method_name}\\nTop {k} Features', fontweight='bold', fontsize=14)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # A√±adir valores\n",
    "        for i, (bar, score) in enumerate(zip(bars, top_features['Score'].values)):\n",
    "            width = bar.get_width()\n",
    "            ax.text(width, bar.get_y() + bar.get_height()/2,\n",
    "                   f' {score:.2f}', ha='left', va='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('M√©todos Filter: Ranking de Features', fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Imprimir rankings\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TOP 10 FEATURES POR M√âTODO\")\n",
    "    print(\"=\"*80)\n",
    "    for method_name, scores_df in results.items():\n",
    "        print(f\"\\n{method_name}:\")\n",
    "        print(scores_df.head(10)[['Feature', 'Score']].to_string(index=False))\n",
    "    \n",
    "    return results, fig\n",
    "\n",
    "# Aplicar m√©todos filter\n",
    "filter_results, fig_filter = apply_filter_methods(\n",
    "    pd.DataFrame(X_cancer_scaled, columns=X_cancer.columns), \n",
    "    y_cancer, \n",
    "    k=15\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 An√°lisis de Correlaciones\n",
    "\n",
    "Identificar features altamente correlacionados (redundantes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_correlations(X, top_features=None, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Visualiza correlaciones entre features y detecta redundancia\n",
    "    \"\"\"\n",
    "    if top_features is not None:\n",
    "        X_subset = X[top_features]\n",
    "    else:\n",
    "        X_subset = X\n",
    "    \n",
    "    # Calcular correlaciones\n",
    "    corr_matrix = X_subset.corr()\n",
    "    \n",
    "    # Encontrar pares altamente correlacionados\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                high_corr_pairs.append({\n",
    "                    'Feature1': corr_matrix.columns[i],\n",
    "                    'Feature2': corr_matrix.columns[j],\n",
    "                    'Correlation': corr_matrix.iloc[i, j]\n",
    "                })\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig = plt.figure(figsize=(18, 14))\n",
    "    \n",
    "    # Heatmap completo\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "                cmap='coolwarm', center=0, square=True,\n",
    "                linewidths=0.5, cbar_kws={'label': 'Correlaci√≥n'},\n",
    "                ax=ax1, vmin=-1, vmax=1)\n",
    "    ax1.set_title('Matriz de Correlaci√≥n entre Features', fontweight='bold', fontsize=16)\n",
    "    \n",
    "    # Distribuci√≥n de correlaciones\n",
    "    ax2 = plt.subplot(2, 2, 3)\n",
    "    corr_values = corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)]\n",
    "    ax2.hist(corr_values, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    ax2.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Umbral: ¬±{threshold}')\n",
    "    ax2.axvline(-threshold, color='red', linestyle='--', linewidth=2)\n",
    "    ax2.set_xlabel('Correlaci√≥n', fontsize=12)\n",
    "    ax2.set_ylabel('Frecuencia', fontsize=12)\n",
    "    ax2.set_title('Distribuci√≥n de Correlaciones', fontweight='bold', fontsize=14)\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    # Tabla de features altamente correlacionados\n",
    "    ax3 = plt.subplot(2, 2, 4)\n",
    "    ax3.axis('tight')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        df_high_corr = pd.DataFrame(high_corr_pairs)\n",
    "        df_high_corr = df_high_corr.sort_values('Correlation', ascending=False, key=abs)\n",
    "        \n",
    "        table_data = []\n",
    "        for _, row in df_high_corr.head(15).iterrows():\n",
    "            table_data.append([\n",
    "                row['Feature1'][:20],\n",
    "                row['Feature2'][:20],\n",
    "                f\"{row['Correlation']:.3f}\"\n",
    "            ])\n",
    "        \n",
    "        table = ax3.table(cellText=table_data,\n",
    "                         colLabels=['Feature 1', 'Feature 2', 'Corr'],\n",
    "                         cellLoc='left',\n",
    "                         loc='center',\n",
    "                         colWidths=[0.4, 0.4, 0.2])\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(9)\n",
    "        table.scale(1, 2)\n",
    "        \n",
    "        # Colorear header\n",
    "        for i in range(3):\n",
    "            table[(0, i)].set_facecolor('#40466e')\n",
    "            table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "        \n",
    "        ax3.set_title(f'Features Altamente Correlacionados (|r| > {threshold})\\n{len(high_corr_pairs)} pares encontrados',\n",
    "                     fontweight='bold', fontsize=14, pad=20)\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, f'No hay features con |r| > {threshold}',\n",
    "                ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"AN√ÅLISIS DE CORRELACIONES (umbral = {threshold})\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal de pares altamente correlacionados: {len(high_corr_pairs)}\")\n",
    "    if high_corr_pairs:\n",
    "        print(\"\\nTop 10 pares m√°s correlacionados:\")\n",
    "        df_high_corr = pd.DataFrame(high_corr_pairs).sort_values('Correlation', ascending=False, key=abs)\n",
    "        print(df_high_corr.head(10).to_string(index=False))\n",
    "    \n",
    "    return fig, high_corr_pairs\n",
    "\n",
    "# Analizar correlaciones en top features de F-test\n",
    "top_15_features = filter_results['F-test'].head(15)['Feature'].tolist()\n",
    "fig_corr, high_corr = plot_feature_correlations(\n",
    "    pd.DataFrame(X_cancer_scaled, columns=X_cancer.columns),\n",
    "    top_features=top_15_features,\n",
    "    threshold=0.8\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 M√©todos Wrapper\n",
    "\n",
    "Eval√∫an subconjuntos de features entrenando modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_wrapper_methods(X, y, n_features_to_select=10):\n",
    "    \"\"\"\n",
    "    Aplica RFE (Recursive Feature Elimination) con diferentes modelos\n",
    "    \"\"\"\n",
    "    feature_names = X.columns if hasattr(X, 'columns') else [f'F{i}' for i in range(X.shape[1])]\n",
    "    \n",
    "    # Definir modelos\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"M√âTODOS WRAPPER - RFE (Recursive Feature Elimination)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nSeleccionando top {n_features_to_select} features con cada modelo...\")\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n{model_name}...\", end=' ')\n",
    "        \n",
    "        # RFE\n",
    "        rfe = RFE(estimator=model, n_features_to_select=n_features_to_select, step=1)\n",
    "        rfe.fit(X, y)\n",
    "        \n",
    "        # Guardar resultados\n",
    "        results[model_name] = {\n",
    "            'selected': feature_names[rfe.support_].tolist(),\n",
    "            'ranking': rfe.ranking_\n",
    "        }\n",
    "        \n",
    "        print(\"‚úì\")\n",
    "        print(f\"  Features seleccionados: {results[model_name]['selected'][:5]}...\")\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # 1. Ranking por modelo\n",
    "    for idx, (model_name, result) in enumerate(results.items(), 1):\n",
    "        ax = plt.subplot(2, 3, idx)\n",
    "        \n",
    "        ranking_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Ranking': result['ranking']\n",
    "        }).sort_values('Ranking')\n",
    "        \n",
    "        top_features = ranking_df.head(15)\n",
    "        colors = ['green' if r == 1 else 'orange' if r <= 3 else 'red' \n",
    "                 for r in top_features['Ranking']]\n",
    "        \n",
    "        bars = ax.barh(range(len(top_features)), top_features['Ranking'].values, \n",
    "                      color=colors, alpha=0.7, edgecolor='black')\n",
    "        ax.set_yticks(range(len(top_features)))\n",
    "        ax.set_yticklabels(top_features['Feature'].values, fontsize=9)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_xlabel('Ranking (1 = mejor)', fontsize=11)\n",
    "        ax.set_title(f'{model_name}\\nTop 15 Features', fontweight='bold', fontsize=13)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # A√±adir l√≠nea en ranking = n_features_to_select\n",
    "        ax.axvline(n_features_to_select, color='blue', linestyle='--', \n",
    "                  linewidth=2, alpha=0.5, label=f'Top {n_features_to_select}')\n",
    "        ax.legend()\n",
    "    \n",
    "    # 2. Diagrama de Venn (consenso)\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    selected_sets = {name: set(result['selected']) for name, result in results.items()}\n",
    "    \n",
    "    # Intersecciones\n",
    "    all_three = selected_sets['Logistic Regression'] & selected_sets['Random Forest'] & selected_sets['Gradient Boosting']\n",
    "    lr_rf = (selected_sets['Logistic Regression'] & selected_sets['Random Forest']) - all_three\n",
    "    lr_gb = (selected_sets['Logistic Regression'] & selected_sets['Gradient Boosting']) - all_three\n",
    "    rf_gb = (selected_sets['Random Forest'] & selected_sets['Gradient Boosting']) - all_three\n",
    "    \n",
    "    only_lr = selected_sets['Logistic Regression'] - selected_sets['Random Forest'] - selected_sets['Gradient Boosting']\n",
    "    only_rf = selected_sets['Random Forest'] - selected_sets['Logistic Regression'] - selected_sets['Gradient Boosting']\n",
    "    only_gb = selected_sets['Gradient Boosting'] - selected_sets['Logistic Regression'] - selected_sets['Random Forest']\n",
    "    \n",
    "    # Texto\n",
    "    y_pos = 0.9\n",
    "    ax4.text(0.5, y_pos, 'CONSENSO ENTRE MODELOS', ha='center', fontsize=16, fontweight='bold')\n",
    "    y_pos -= 0.1\n",
    "    \n",
    "    ax4.text(0.1, y_pos, f'üü¢ Los 3 modelos ({len(all_three)}):', fontsize=12, fontweight='bold')\n",
    "    y_pos -= 0.05\n",
    "    for feat in sorted(all_three):\n",
    "        ax4.text(0.15, y_pos, f'‚Ä¢ {feat}', fontsize=10)\n",
    "        y_pos -= 0.04\n",
    "    \n",
    "    y_pos -= 0.03\n",
    "    ax4.text(0.1, y_pos, f'üü° 2 modelos:', fontsize=12, fontweight='bold')\n",
    "    y_pos -= 0.05\n",
    "    for feat in sorted(lr_rf | lr_gb | rf_gb):\n",
    "        ax4.text(0.15, y_pos, f'‚Ä¢ {feat}', fontsize=10)\n",
    "        y_pos -= 0.04\n",
    "        if y_pos < 0.1:\n",
    "            break\n",
    "    \n",
    "    ax4.set_xlim(0, 1)\n",
    "    ax4.set_ylim(0, 1)\n",
    "    \n",
    "    # 3. Heatmap de selecci√≥n\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    selection_matrix = []\n",
    "    model_names_list = list(results.keys())\n",
    "    \n",
    "    for model_name in model_names_list:\n",
    "        row = [1 if feat in results[model_name]['selected'] else 0 \n",
    "               for feat in feature_names]\n",
    "        selection_matrix.append(row)\n",
    "    \n",
    "    selection_df = pd.DataFrame(selection_matrix, \n",
    "                               index=model_names_list,\n",
    "                               columns=feature_names)\n",
    "    \n",
    "    # Ordenar por n√∫mero de selecciones\n",
    "    feature_counts = selection_df.sum(axis=0)\n",
    "    selection_df = selection_df[feature_counts.sort_values(ascending=False).index]\n",
    "    \n",
    "    sns.heatmap(selection_df.iloc[:, :20], annot=True, fmt='d', cmap='RdYlGn',\n",
    "                cbar_kws={'label': 'Seleccionado'}, ax=ax5,\n",
    "                linewidths=0.5, vmin=0, vmax=1)\n",
    "    ax5.set_title('Features Seleccionados por Modelo\\n(Top 20 m√°s frecuentes)', \n",
    "                 fontweight='bold', fontsize=13)\n",
    "    ax5.set_xlabel('')\n",
    "    ax5.set_ylabel('')\n",
    "    \n",
    "    # 4. Frecuencia de selecci√≥n\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    feature_counts_sorted = feature_counts.sort_values(ascending=False).head(15)\n",
    "    colors_freq = ['green' if c == 3 else 'orange' if c == 2 else 'red' \n",
    "                   for c in feature_counts_sorted.values]\n",
    "    \n",
    "    bars = ax6.barh(range(len(feature_counts_sorted)), feature_counts_sorted.values,\n",
    "                   color=colors_freq, alpha=0.7, edgecolor='black')\n",
    "    ax6.set_yticks(range(len(feature_counts_sorted)))\n",
    "    ax6.set_yticklabels(feature_counts_sorted.index, fontsize=10)\n",
    "    ax6.invert_yaxis()\n",
    "    ax6.set_xlabel('N√∫mero de modelos que lo seleccionaron', fontsize=11)\n",
    "    ax6.set_title('Frecuencia de Selecci√≥n\\nTop 15 Features', fontweight='bold', fontsize=13)\n",
    "    ax6.set_xticks([0, 1, 2, 3])\n",
    "    ax6.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('M√©todos Wrapper: RFE con M√∫ltiples Modelos', \n",
    "                fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return results, all_three, fig\n",
    "\n",
    "# Aplicar RFE\n",
    "wrapper_results, consensus_features, fig_wrapper = apply_wrapper_methods(\n",
    "    pd.DataFrame(X_cancer_scaled, columns=X_cancer.columns),\n",
    "    y_cancer,\n",
    "    n_features_to_select=10\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURES CON CONSENSO (seleccionados por los 3 modelos):\")\n",
    "print(\"=\"*80)\n",
    "for feat in sorted(consensus_features):\n",
    "    print(f\"  ‚úì {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Parte 4: Balanceo de Clases\n",
    "\n",
    "Manejaremos el desbalance de clases usando t√©cnicas de over/undersampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Creaci√≥n de Dataset Desbalanceado\n",
    "\n",
    "Simularemos un escenario realista de desbalance severo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset desbalanceado\n",
    "def create_imbalanced_dataset(n_samples=1000, imbalance_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Crea dataset de clasificaci√≥n con desbalance de clases\n",
    "    \"\"\"\n",
    "    # Clase mayoritaria\n",
    "    n_majority = int(n_samples * (1 - imbalance_ratio))\n",
    "    n_minority = n_samples - n_majority\n",
    "    \n",
    "    X_imb, y_imb = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=20,\n",
    "        n_informative=15,\n",
    "        n_redundant=5,\n",
    "        n_classes=2,\n",
    "        weights=[1-imbalance_ratio, imbalance_ratio],\n",
    "        flip_y=0.01,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    feature_names = [f'feature_{i}' for i in range(X_imb.shape[1])]\n",
    "    X_imb_df = pd.DataFrame(X_imb, columns=feature_names)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"DATASET DESBALANCEADO CREADO\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal de observaciones: {len(y_imb)}\")\n",
    "    unique, counts = np.unique(y_imb, return_counts=True)\n",
    "    for cls, count in zip(unique, counts):\n",
    "        pct = 100 * count / len(y_imb)\n",
    "        print(f\"  Clase {cls}: {count:4d} ({pct:5.2f}%)\")\n",
    "    \n",
    "    ratio = counts[1] / counts[0]\n",
    "    print(f\"\\nRatio Minor√≠a/Mayor√≠a: {ratio:.3f} ({ratio:.1%})\")\n",
    "    \n",
    "    return X_imb_df, y_imb\n",
    "\n",
    "# Crear dataset\n",
    "X_imb, y_imb = create_imbalanced_dataset(n_samples=1000, imbalance_ratio=0.10)\n",
    "\n",
    "# Visualizar distribuci√≥n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribuci√≥n de clases\n",
    "ax = axes[0]\n",
    "unique, counts = np.unique(y_imb, return_counts=True)\n",
    "bars = ax.bar(unique, counts, color=['red', 'green'], alpha=0.7, edgecolor='black', width=0.6)\n",
    "ax.set_xlabel('Clase', fontsize=13)\n",
    "ax.set_ylabel('N√∫mero de observaciones', fontsize=13)\n",
    "ax.set_title('Distribuci√≥n Original de Clases', fontweight='bold', fontsize=15)\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['Clase 0\\n(Mayor√≠a)', 'Clase 1\\n(Minor√≠a)'])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, count in zip(bars, counts):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "           f'{count}\\n({100*count/len(y_imb):.1f}%)',\n",
    "           ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# PCA del dataset desbalanceado\n",
    "ax = axes[1]\n",
    "pca_imb = PCA(n_components=2)\n",
    "X_imb_pca = pca_imb.fit_transform(X_imb)\n",
    "\n",
    "scatter = ax.scatter(X_imb_pca[:, 0], X_imb_pca[:, 1], \n",
    "                    c=y_imb, cmap='RdYlGn', alpha=0.6, s=50,\n",
    "                    edgecolors='black', linewidth=0.5)\n",
    "ax.set_xlabel(f'PC1 ({pca_imb.explained_variance_ratio_[0]:.1%})', fontsize=12)\n",
    "ax.set_ylabel(f'PC2 ({pca_imb.explained_variance_ratio_[1]:.1%})', fontsize=12)\n",
    "ax.set_title('Visualizaci√≥n PCA del Dataset Desbalanceado', fontweight='bold', fontsize=15)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax, label='Clase', ticks=[0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Comparaci√≥n de M√©todos de Balanceo\n",
    "\n",
    "Compararemos diferentes t√©cnicas de over/undersampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_balancing_methods(X, y):\n",
    "    \"\"\"\n",
    "    Compara m√∫ltiples m√©todos de balanceo de clases\n",
    "    \"\"\"\n",
    "    methods = {}\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"APLICANDO M√âTODOS DE BALANCEO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Original (no balancing)\n",
    "    methods['1. Original'] = (X.copy(), y.copy())\n",
    "    print(\"‚úì 1. Original (sin balanceo)\")\n",
    "    \n",
    "    # 2. Random Oversampling\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_ros, y_ros = ros.fit_resample(X, y)\n",
    "    methods['2. Random\\nOversampling'] = (X_ros, y_ros)\n",
    "    print(\"‚úì 2. Random Oversampling\")\n",
    "    \n",
    "    # 3. SMOTE\n",
    "    smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "    X_smote, y_smote = smote.fit_resample(X, y)\n",
    "    methods['3. SMOTE'] = (X_smote, y_smote)\n",
    "    print(\"‚úì 3. SMOTE (Synthetic Minority Over-sampling)\")\n",
    "    \n",
    "    # 4. ADASYN\n",
    "    try:\n",
    "        adasyn = ADASYN(random_state=42, n_neighbors=5)\n",
    "        X_adasyn, y_adasyn = adasyn.fit_resample(X, y)\n",
    "        methods['4. ADASYN'] = (X_adasyn, y_adasyn)\n",
    "        print(\"‚úì 4. ADASYN (Adaptive Synthetic)\")\n",
    "    except:\n",
    "        print(\"‚ö† 4. ADASYN - No aplicable (muy pocos ejemplos minoritarios)\")\n",
    "    \n",
    "    # 5. BorderlineSMOTE\n",
    "    try:\n",
    "        bsmote = BorderlineSMOTE(random_state=42, k_neighbors=5)\n",
    "        X_bsmote, y_bsmote = bsmote.fit_resample(X, y)\n",
    "        methods['5. Borderline\\nSMOTE'] = (X_bsmote, y_bsmote)\n",
    "        print(\"‚úì 5. BorderlineSMOTE\")\n",
    "    except:\n",
    "        print(\"‚ö† 5. BorderlineSMOTE - No aplicable\")\n",
    "    \n",
    "    # 6. Random Undersampling\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_rus, y_rus = rus.fit_resample(X, y)\n",
    "    methods['6. Random\\nUndersampling'] = (X_rus, y_rus)\n",
    "    print(\"‚úì 6. Random Undersampling\")\n",
    "    \n",
    "    # 7. SMOTE + Tomek Links\n",
    "    try:\n",
    "        smote_tomek = SMOTETomek(random_state=42)\n",
    "        X_st, y_st = smote_tomek.fit_resample(X, y)\n",
    "        methods['7. SMOTE +\\nTomek'] = (X_st, y_st)\n",
    "        print(\"‚úì 7. SMOTE + Tomek Links (Hybrid)\")\n",
    "    except:\n",
    "        print(\"‚ö† 7. SMOTE + Tomek - No aplicable\")\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    n_methods = len(methods)\n",
    "    n_cols = 4\n",
    "    n_rows = (n_methods + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 5*n_rows))\n",
    "    \n",
    "    for idx, (method_name, (X_bal, y_bal)) in enumerate(methods.items(), 1):\n",
    "        # Distribuci√≥n de clases\n",
    "        ax = plt.subplot(n_rows, n_cols, idx)\n",
    "        \n",
    "        unique, counts = np.unique(y_bal, return_counts=True)\n",
    "        bars = ax.bar(unique, counts, alpha=0.7, \n",
    "                     color=['red', 'green'], edgecolor='black', width=0.6)\n",
    "        \n",
    "        ax.set_xlabel('Clase', fontsize=11)\n",
    "        ax.set_ylabel('N√∫mero de muestras', fontsize=11)\n",
    "        \n",
    "        ratio = counts[1] / counts[0] if len(counts) > 1 else 0\n",
    "        total_samples = len(y_bal)\n",
    "        \n",
    "        title = f'{method_name}\\nN={total_samples} | Ratio={ratio:.2f}'\n",
    "        ax.set_title(title, fontweight='bold', fontsize=12)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        ax.set_xticks([0, 1])\n",
    "        \n",
    "        # A√±adir etiquetas en barras\n",
    "        for bar, count in zip(bars, counts):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{count}\\n({100*count/total_samples:.1f}%)',\n",
    "                   ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Comparaci√≥n de M√©todos de Balanceo de Clases', \n",
    "                fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return methods, fig\n",
    "\n",
    "# Aplicar m√©todos\n",
    "balancing_results, fig_balance = compare_balancing_methods(X_imb, y_imb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Visualizaci√≥n del Impacto del Balanceo\n",
    "\n",
    "Ver c√≥mo cada m√©todo afecta el espacio de features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_balancing_impact(X_original, y_original, balancing_results):\n",
    "    \"\"\"\n",
    "    Visualiza el impacto de cada m√©todo de balanceo en el espacio PCA\n",
    "    \"\"\"\n",
    "    # Ajustar PCA en datos originales\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X_original)\n",
    "    \n",
    "    n_methods = len(balancing_results)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_methods + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 6*n_rows))\n",
    "    \n",
    "    for idx, (method_name, (X_bal, y_bal)) in enumerate(balancing_results.items(), 1):\n",
    "        ax = plt.subplot(n_rows, n_cols, idx)\n",
    "        \n",
    "        # Proyectar datos balanceados con PCA original\n",
    "        X_bal_pca = pca.transform(X_bal)\n",
    "        \n",
    "        # Separar clases para visualizaci√≥n\n",
    "        mask_0 = y_bal == 0\n",
    "        mask_1 = y_bal == 1\n",
    "        \n",
    "        ax.scatter(X_bal_pca[mask_0, 0], X_bal_pca[mask_0, 1],\n",
    "                  c='red', alpha=0.4, s=30, label='Clase 0', edgecolors='none')\n",
    "        ax.scatter(X_bal_pca[mask_1, 0], X_bal_pca[mask_1, 1],\n",
    "                  c='green', alpha=0.6, s=30, label='Clase 1', edgecolors='none')\n",
    "        \n",
    "        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})', fontsize=11)\n",
    "        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})', fontsize=11)\n",
    "        \n",
    "        n_samples = len(y_bal)\n",
    "        n_class_1 = (y_bal == 1).sum()\n",
    "        ax.set_title(f'{method_name}\\nN={n_samples} | Clase 1: {n_class_1}',\n",
    "                    fontweight='bold', fontsize=13)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.legend(loc='upper right', fontsize=9)\n",
    "    \n",
    "    plt.suptitle('Impacto del Balanceo en el Espacio de Features (PCA)', \n",
    "                fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig_impact = visualize_balancing_impact(X_imb, y_imb, balancing_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Evaluaci√≥n del Impacto en el Desempe√±o\n",
    "\n",
    "Comparar m√©tricas de clasificaci√≥n con cada m√©todo de balanceo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_balancing_methods(X_original, y_original, balancing_results):\n",
    "    \"\"\"\n",
    "    Eval√∫a el desempe√±o de clasificaci√≥n con cada m√©todo de balanceo\n",
    "    \"\"\"\n",
    "    # Split original data\n",
    "    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "        X_original, y_original, test_size=0.3, random_state=42, stratify=y_original\n",
    "    )\n",
    "    \n",
    "    results_metrics = []\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"EVALUACI√ìN DE M√âTODOS DE BALANCEO\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nEntrenando modelos con cada m√©todo de balanceo...\")\n",
    "    \n",
    "    for method_name, (X_bal, y_bal) in balancing_results.items():\n",
    "        print(f\"  {method_name}...\", end=' ')\n",
    "        \n",
    "        # Para el m√©todo original, usar train/test split normal\n",
    "        if 'Original' in method_name:\n",
    "            X_train, y_train = X_train_orig, y_train_orig\n",
    "        else:\n",
    "            # Para m√©todos de balanceo, aplicar solo a training set\n",
    "            if len(X_bal) > len(X_train_orig):\n",
    "                # Es oversampling, tomar muestra del tama√±o apropiado\n",
    "                indices = np.random.choice(len(X_bal), size=len(X_train_orig)*2, replace=False)\n",
    "                X_train = X_bal.iloc[indices] if hasattr(X_bal, 'iloc') else X_bal[indices]\n",
    "                y_train = y_bal[indices]\n",
    "            else:\n",
    "                X_train, y_train = X_bal, y_bal\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predecir en test set (siempre el mismo)\n",
    "        y_pred = model.predict(X_test_orig)\n",
    "        y_pred_proba = model.predict_proba(X_test_orig)[:, 1]\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        metrics = {\n",
    "            'Method': method_name.replace('\\n', ' '),\n",
    "            'Accuracy': accuracy_score(y_test_orig, y_pred),\n",
    "            'Precision': precision_score(y_test_orig, y_pred, zero_division=0),\n",
    "            'Recall': recall_score(y_test_orig, y_pred, zero_division=0),\n",
    "            'F1-Score': f1_score(y_test_orig, y_pred, zero_division=0),\n",
    "            'ROC-AUC': roc_auc_score(y_test_orig, y_pred_proba)\n",
    "        }\n",
    "        \n",
    "        results_metrics.append(metrics)\n",
    "        print(\"‚úì\")\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    df_metrics = pd.DataFrame(results_metrics)\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    # 1. Todas las m√©tricas\n",
    "    ax1 = plt.subplot(2, 2, 1)\n",
    "    df_plot = df_metrics.set_index('Method')[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']]\n",
    "    df_plot.plot(kind='bar', ax=ax1, width=0.8, edgecolor='black')\n",
    "    ax1.set_ylabel('Score', fontsize=12)\n",
    "    ax1.set_title('Comparaci√≥n de Todas las M√©tricas', fontweight='bold', fontsize=14)\n",
    "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax1.legend(loc='lower right', fontsize=10)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    ax1.set_ylim(0, 1.05)\n",
    "    ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    # 2. Precision vs Recall\n",
    "    ax2 = plt.subplot(2, 2, 2)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(df_metrics)))\n",
    "    for idx, row in df_metrics.iterrows():\n",
    "        ax2.scatter(row['Recall'], row['Precision'], s=200, alpha=0.7,\n",
    "                   color=colors[idx], edgecolors='black', linewidth=2)\n",
    "        ax2.annotate(row['Method'], \n",
    "                    (row['Recall'], row['Precision']),\n",
    "                    fontsize=9, ha='center')\n",
    "    \n",
    "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "    ax2.set_xlabel('Recall', fontsize=12)\n",
    "    ax2.set_ylabel('Precision', fontsize=12)\n",
    "    ax2.set_title('Precision vs Recall Trade-off', fontweight='bold', fontsize=14)\n",
    "    ax2.grid(alpha=0.3)\n",
    "    ax2.set_xlim(-0.05, 1.05)\n",
    "    ax2.set_ylim(-0.05, 1.05)\n",
    "    \n",
    "    # 3. F1-Score ranking\n",
    "    ax3 = plt.subplot(2, 2, 3)\n",
    "    df_sorted = df_metrics.sort_values('F1-Score')\n",
    "    colors_f1 = plt.cm.RdYlGn(df_sorted['F1-Score'].values)\n",
    "    bars = ax3.barh(range(len(df_sorted)), df_sorted['F1-Score'].values,\n",
    "                    color=colors_f1, edgecolor='black')\n",
    "    ax3.set_yticks(range(len(df_sorted)))\n",
    "    ax3.set_yticklabels(df_sorted['Method'].values, fontsize=10)\n",
    "    ax3.set_xlabel('F1-Score', fontsize=12)\n",
    "    ax3.set_title('Ranking por F1-Score', fontweight='bold', fontsize=14)\n",
    "    ax3.grid(axis='x', alpha=0.3)\n",
    "    ax3.set_xlim(0, 1)\n",
    "    \n",
    "    for bar, score in zip(bars, df_sorted['F1-Score'].values):\n",
    "        width = bar.get_width()\n",
    "        ax3.text(width + 0.02, bar.get_y() + bar.get_height()/2,\n",
    "                f'{score:.3f}', ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 4. Tabla de resultados\n",
    "    ax4 = plt.subplot(2, 2, 4)\n",
    "    ax4.axis('tight')\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    table_data = []\n",
    "    for _, row in df_metrics.iterrows():\n",
    "        table_data.append([\n",
    "            row['Method'][:20],\n",
    "            f\"{row['Accuracy']:.3f}\",\n",
    "            f\"{row['Precision']:.3f}\",\n",
    "            f\"{row['Recall']:.3f}\",\n",
    "            f\"{row['F1-Score']:.3f}\",\n",
    "            f\"{row['ROC-AUC']:.3f}\"\n",
    "        ])\n",
    "    \n",
    "    table = ax4.table(cellText=table_data,\n",
    "                     colLabels=['Method', 'Acc', 'Prec', 'Rec', 'F1', 'AUC'],\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     colWidths=[0.35, 0.13, 0.13, 0.13, 0.13, 0.13])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # Colorear header\n",
    "    for i in range(6):\n",
    "        table[(0, i)].set_facecolor('#40466e')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Colorear mejor de cada columna\n",
    "    for col_idx in range(1, 6):\n",
    "        col_values = [float(table_data[i][col_idx]) for i in range(len(table_data))]\n",
    "        best_idx = np.argmax(col_values)\n",
    "        table[(best_idx + 1, col_idx)].set_facecolor('#90EE90')\n",
    "        table[(best_idx + 1, col_idx)].set_text_props(weight='bold')\n",
    "    \n",
    "    ax4.set_title('Tabla de Resultados\\n(Verde = Mejor en cada m√©trica)', \n",
    "                 fontweight='bold', fontsize=14, pad=20)\n",
    "    \n",
    "    plt.suptitle('Evaluaci√≥n del Impacto del Balanceo en el Desempe√±o', \n",
    "                fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESULTADOS:\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_metrics.to_string(index=False))\n",
    "    \n",
    "    # Identificar mejor m√©todo\n",
    "    best_f1 = df_metrics.loc[df_metrics['F1-Score'].idxmax()]\n",
    "    print(f\"\\nüèÜ Mejor m√©todo por F1-Score: {best_f1['Method']} ({best_f1['F1-Score']:.3f})\")\n",
    "    \n",
    "    return df_metrics, fig\n",
    "\n",
    "# Evaluar m√©todos\n",
    "metrics_df, fig_eval = evaluate_balancing_methods(X_imb, y_imb, balancing_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Parte 5: Pipeline Completo de Preprocesamiento\n",
    "\n",
    "Integraremos todas las t√©cnicas en un pipeline reproducible y robusto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Construcci√≥n del Pipeline\n",
    "\n",
    "Crearemos un pipeline modular que integre todas las etapas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataCleaningTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer personalizado para limpieza de datos\n",
    "    \"\"\"\n",
    "    def __init__(self, imputation_strategy='knn', outlier_method='cap'):\n",
    "        self.imputation_strategy = imputation_strategy\n",
    "        self.outlier_method = outlier_method\n",
    "        self.imputer = None\n",
    "        self.outlier_bounds = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Ajustar imputer\n",
    "        if self.imputation_strategy == 'knn':\n",
    "            self.imputer = KNNImputer(n_neighbors=5)\n",
    "        elif self.imputation_strategy == 'median':\n",
    "            self.imputer = SimpleImputer(strategy='median')\n",
    "        else:\n",
    "            self.imputer = SimpleImputer(strategy='mean')\n",
    "        \n",
    "        self.imputer.fit(X)\n",
    "        \n",
    "        # Calcular l√≠mites de outliers\n",
    "        if self.outlier_method == 'cap':\n",
    "            X_clean = self.imputer.transform(X)\n",
    "            for i in range(X_clean.shape[1]):\n",
    "                q05 = np.percentile(X_clean[:, i], 5)\n",
    "                q95 = np.percentile(X_clean[:, i], 95)\n",
    "                self.outlier_bounds[i] = (q05, q95)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Imputar\n",
    "        X_clean = self.imputer.transform(X)\n",
    "        \n",
    "        # Tratar outliers\n",
    "        if self.outlier_method == 'cap':\n",
    "            for i, (lower, upper) in self.outlier_bounds.items():\n",
    "                X_clean[:, i] = np.clip(X_clean[:, i], lower, upper)\n",
    "        \n",
    "        return X_clean\n",
    "\n",
    "def create_preprocessing_pipeline(\n",
    "    use_cleaning=True,\n",
    "    use_scaling=True,\n",
    "    use_pca=False,\n",
    "    use_feature_selection=False,\n",
    "    use_balancing=False,\n",
    "    n_components_pca=0.95,\n",
    "    n_features=10,\n",
    "    balancing_method='smote'\n",
    "):\n",
    "    \"\"\"\n",
    "    Crea pipeline de preprocesamiento configurable\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    \n",
    "    # 1. Limpieza (opcional)\n",
    "    if use_cleaning:\n",
    "        steps.append(('cleaning', DataCleaningTransformer(\n",
    "            imputation_strategy='knn',\n",
    "            outlier_method='cap'\n",
    "        )))\n",
    "    \n",
    "    # 2. Escalamiento (opcional)\n",
    "    if use_scaling:\n",
    "        steps.append(('scaler', StandardScaler()))\n",
    "    \n",
    "    # 3. Reducci√≥n de dimensionalidad (opcional)\n",
    "    if use_pca:\n",
    "        steps.append(('pca', PCA(n_components=n_components_pca)))\n",
    "    \n",
    "    # 4. Selecci√≥n de atributos (opcional)\n",
    "    if use_feature_selection:\n",
    "        steps.append(('feature_selection', SelectKBest(\n",
    "            f_classif, k=n_features\n",
    "        )))\n",
    "    \n",
    "    # 5. Balanceo (opcional)\n",
    "    if use_balancing:\n",
    "        if balancing_method == 'smote':\n",
    "            steps.append(('balancing', SMOTE(random_state=42)))\n",
    "        elif balancing_method == 'adasyn':\n",
    "            steps.append(('balancing', ADASYN(random_state=42)))\n",
    "        elif balancing_method == 'borderline':\n",
    "            steps.append(('balancing', BorderlineSMOTE(random_state=42)))\n",
    "    \n",
    "    # 6. Clasificador\n",
    "    steps.append(('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )))\n",
    "    \n",
    "    pipeline = Pipeline(steps)\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EJEMPLO DE PIPELINE COMPLETO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Crear pipeline\n",
    "pipeline = create_preprocessing_pipeline(\n",
    "    use_cleaning=True,\n",
    "    use_scaling=True,\n",
    "    use_pca=True,\n",
    "    use_feature_selection=False,\n",
    "    use_balancing=True,\n",
    "    n_components_pca=0.95,\n",
    "    balancing_method='smote'\n",
    ")\n",
    "\n",
    "print(\"\\nPasos del pipeline:\")\n",
    "for name, step in pipeline.steps:\n",
    "    print(f\"  {name:20s}: {step.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Evaluaci√≥n del Pipeline\n",
    "\n",
    "Comparemos diferentes configuraciones del pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline_configurations(X, y):\n",
    "    \"\"\"\n",
    "    Eval√∫a m√∫ltiples configuraciones del pipeline\n",
    "    \"\"\"\n",
    "    # Configuraciones a probar\n",
    "    configs = [\n",
    "        {\n",
    "            'name': 'Baseline (solo clasificador)',\n",
    "            'use_cleaning': False,\n",
    "            'use_scaling': False,\n",
    "            'use_pca': False,\n",
    "            'use_feature_selection': False,\n",
    "            'use_balancing': False\n",
    "        },\n",
    "        {\n",
    "            'name': 'Escalamiento solo',\n",
    "            'use_cleaning': False,\n",
    "            'use_scaling': True,\n",
    "            'use_pca': False,\n",
    "            'use_feature_selection': False,\n",
    "            'use_balancing': False\n",
    "        },\n",
    "        {\n",
    "            'name': 'Escalamiento + PCA',\n",
    "            'use_cleaning': False,\n",
    "            'use_scaling': True,\n",
    "            'use_pca': True,\n",
    "            'use_feature_selection': False,\n",
    "            'use_balancing': False\n",
    "        },\n",
    "        {\n",
    "            'name': 'Escalamiento + Feature Selection',\n",
    "            'use_cleaning': False,\n",
    "            'use_scaling': True,\n",
    "            'use_pca': False,\n",
    "            'use_feature_selection': True,\n",
    "            'use_balancing': False\n",
    "        },\n",
    "        {\n",
    "            'name': 'Escalamiento + SMOTE',\n",
    "            'use_cleaning': False,\n",
    "            'use_scaling': True,\n",
    "            'use_pca': False,\n",
    "            'use_feature_selection': False,\n",
    "            'use_balancing': True\n",
    "        },\n",
    "        {\n",
    "            'name': 'Pipeline Completo',\n",
    "            'use_cleaning': True,\n",
    "            'use_scaling': True,\n",
    "            'use_pca': True,\n",
    "            'use_feature_selection': False,\n",
    "            'use_balancing': True\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"EVALUANDO CONFIGURACIONES DE PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Split datos\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\n{config['name']}...\", end=' ')\n",
    "        \n",
    "        # Crear pipeline\n",
    "        name = config.pop('name')\n",
    "        pipeline = create_preprocessing_pipeline(**config)\n",
    "        \n",
    "        try:\n",
    "            # Entrenar\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Predecir\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # M√©tricas\n",
    "            results.append({\n",
    "                'Configuration': name,\n",
    "                'Accuracy': accuracy_score(y_test, y_pred),\n",
    "                'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "                'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "                'F1-Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "                'ROC-AUC': roc_auc_score(y_test, y_pred_proba)\n",
    "            })\n",
    "            \n",
    "            print(\"‚úì\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error: {str(e)[:50]}\")\n",
    "            results.append({\n",
    "                'Configuration': name,\n",
    "                'Accuracy': 0,\n",
    "                'Precision': 0,\n",
    "                'Recall': 0,\n",
    "                'F1-Score': 0,\n",
    "                'ROC-AUC': 0\n",
    "            })\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    # 1. Comparaci√≥n de todas las m√©tricas\n",
    "    ax1 = plt.subplot(2, 2, 1)\n",
    "    df_plot = df_results.set_index('Configuration')[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']]\n",
    "    df_plot.plot(kind='bar', ax=ax1, width=0.8, edgecolor='black')\n",
    "    ax1.set_ylabel('Score', fontsize=12)\n",
    "    ax1.set_title('Comparaci√≥n de M√©tricas por Configuraci√≥n', fontweight='bold', fontsize=14)\n",
    "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right', fontsize=10)\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    ax1.set_ylim(0, 1.05)\n",
    "    \n",
    "    # 2. Heatmap de m√©tricas\n",
    "    ax2 = plt.subplot(2, 2, 2)\n",
    "    metrics_matrix = df_results.set_index('Configuration')[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']]\n",
    "    sns.heatmap(metrics_matrix.T, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "                center=0.5, vmin=0, vmax=1, ax=ax2,\n",
    "                linewidths=0.5, cbar_kws={'label': 'Score'})\n",
    "    ax2.set_title('Heatmap de M√©tricas', fontweight='bold', fontsize=14)\n",
    "    ax2.set_xlabel('')\n",
    "    ax2.set_ylabel('', fontsize=12)\n",
    "    \n",
    "    # 3. Ranking por F1-Score\n",
    "    ax3 = plt.subplot(2, 2, 3)\n",
    "    df_sorted = df_results.sort_values('F1-Score')\n",
    "    colors = plt.cm.RdYlGn(df_sorted['F1-Score'].values)\n",
    "    bars = ax3.barh(range(len(df_sorted)), df_sorted['F1-Score'].values,\n",
    "                    color=colors, edgecolor='black')\n",
    "    ax3.set_yticks(range(len(df_sorted)))\n",
    "    ax3.set_yticklabels(df_sorted['Configuration'].values, fontsize=10)\n",
    "    ax3.set_xlabel('F1-Score', fontsize=12)\n",
    "    ax3.set_title('Ranking por F1-Score', fontweight='bold', fontsize=14)\n",
    "    ax3.grid(axis='x', alpha=0.3)\n",
    "    ax3.set_xlim(0, 1)\n",
    "    \n",
    "    for bar, score in zip(bars, df_sorted['F1-Score'].values):\n",
    "        width = bar.get_width()\n",
    "        ax3.text(width + 0.02, bar.get_y() + bar.get_height()/2,\n",
    "                f'{score:.3f}', ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 4. Mejora relativa respecto al baseline\n",
    "    ax4 = plt.subplot(2, 2, 4)\n",
    "    baseline_f1 = df_results[df_results['Configuration'].str.contains('Baseline')]['F1-Score'].values[0]\n",
    "    df_results['Improvement'] = ((df_results['F1-Score'] - baseline_f1) / baseline_f1) * 100\n",
    "    \n",
    "    df_improvement = df_results[~df_results['Configuration'].str.contains('Baseline')].sort_values('Improvement')\n",
    "    colors_imp = ['red' if x < 0 else 'green' for x in df_improvement['Improvement'].values]\n",
    "    \n",
    "    bars = ax4.barh(range(len(df_improvement)), df_improvement['Improvement'].values,\n",
    "                    color=colors_imp, alpha=0.7, edgecolor='black')\n",
    "    ax4.set_yticks(range(len(df_improvement)))\n",
    "    ax4.set_yticklabels(df_improvement['Configuration'].values, fontsize=10)\n",
    "    ax4.set_xlabel('Mejora en F1-Score (%)', fontsize=12)\n",
    "    ax4.set_title(f'Mejora Relativa vs Baseline\\n(Baseline F1={baseline_f1:.3f})', \n",
    "                 fontweight='bold', fontsize=14)\n",
    "    ax4.axvline(0, color='black', linewidth=1)\n",
    "    ax4.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for bar, improvement in zip(bars, df_improvement['Improvement'].values):\n",
    "        width = bar.get_width()\n",
    "        label_pos = width + (5 if width > 0 else -5)\n",
    "        ha = 'left' if width > 0 else 'right'\n",
    "        ax4.text(label_pos, bar.get_y() + bar.get_height()/2,\n",
    "                f'{improvement:+.1f}%', ha=ha, va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Evaluaci√≥n de Configuraciones del Pipeline', \n",
    "                fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESULTADOS:\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_results[['Configuration', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']].to_string(index=False))\n",
    "    \n",
    "    best_config = df_results.loc[df_results['F1-Score'].idxmax()]\n",
    "    print(f\"\\nüèÜ Mejor configuraci√≥n: {best_config['Configuration']}\")\n",
    "    print(f\"   F1-Score: {best_config['F1-Score']:.3f}\")\n",
    "    print(f\"   Mejora vs Baseline: {best_config['Improvement']:.1f}%\")\n",
    "    \n",
    "    return df_results, fig\n",
    "\n",
    "# Evaluar usando el dataset desbalanceado\n",
    "results_pipeline, fig_pipeline = evaluate_pipeline_configurations(X_imb, y_imb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Validaci√≥n Cruzada del Mejor Pipeline\n",
    "\n",
    "Validaremos la robustez del mejor pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_pipeline(X, y, pipeline=None, cv=5):\n",
    "    \"\"\"\n",
    "    Realiza validaci√≥n cruzada estratificada del pipeline\n",
    "    \"\"\"\n",
    "    if pipeline is None:\n",
    "        pipeline = create_preprocessing_pipeline(\n",
    "            use_cleaning=True,\n",
    "            use_scaling=True,\n",
    "            use_pca=True,\n",
    "            use_balancing=True\n",
    "        )\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"VALIDACI√ìN CRUZADA DEL PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nRealizando validaci√≥n cruzada con {cv} folds...\")\n",
    "    \n",
    "    # Validaci√≥n cruzada estratificada\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    \n",
    "    scores = {\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'roc_auc': []\n",
    "    }\n",
    "    \n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"  Fold {fold}/{cv}...\", end=' ')\n",
    "        \n",
    "        X_train = X.iloc[train_idx] if hasattr(X, 'iloc') else X[train_idx]\n",
    "        X_test = X.iloc[test_idx] if hasattr(X, 'iloc') else X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Entrenar\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Predecir\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        fold_scores = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "        }\n",
    "        \n",
    "        for metric, score in fold_scores.items():\n",
    "            scores[metric].append(score)\n",
    "        \n",
    "        fold_results.append(fold_scores)\n",
    "        print(\"‚úì\")\n",
    "    \n",
    "    # Calcular estad√≠sticas\n",
    "    stats = {}\n",
    "    for metric, values in scores.items():\n",
    "        stats[metric] = {\n",
    "            'mean': np.mean(values),\n",
    "            'std': np.std(values),\n",
    "            'min': np.min(values),\n",
    "            'max': np.max(values)\n",
    "        }\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    # 1. Box plots de m√©tricas\n",
    "    ax1 = plt.subplot(2, 2, 1)\n",
    "    metrics_df = pd.DataFrame(scores)\n",
    "    bp = ax1.boxplot([metrics_df[col].values for col in metrics_df.columns],\n",
    "                     labels=['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
    "                     patch_artist=True, showmeans=True,\n",
    "                     meanprops=dict(marker='D', markerfacecolor='red', markersize=8))\n",
    "    \n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "    \n",
    "    ax1.set_ylabel('Score', fontsize=12)\n",
    "    ax1.set_title(f'Distribuci√≥n de M√©tricas\\n({cv}-Fold Cross-Validation)', \n",
    "                 fontweight='bold', fontsize=14)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    ax1.set_ylim(0, 1.05)\n",
    "    \n",
    "    # 2. M√©tricas por fold\n",
    "    ax2 = plt.subplot(2, 2, 2)\n",
    "    folds = list(range(1, cv+1))\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
    "        ax2.plot(folds, scores[metric], marker='o', label=metric.replace('_', '-').title(), linewidth=2)\n",
    "    \n",
    "    ax2.set_xlabel('Fold', fontsize=12)\n",
    "    ax2.set_ylabel('Score', fontsize=12)\n",
    "    ax2.set_title('M√©tricas por Fold', fontweight='bold', fontsize=14)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(alpha=0.3)\n",
    "    ax2.set_xticks(folds)\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    \n",
    "    # 3. Media y desviaci√≥n est√°ndar\n",
    "    ax3 = plt.subplot(2, 2, 3)\n",
    "    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "    means = [stats[m]['mean'] for m in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']]\n",
    "    stds = [stats[m]['std'] for m in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']]\n",
    "    \n",
    "    x_pos = np.arange(len(metrics_names))\n",
    "    bars = ax3.bar(x_pos, means, yerr=stds, alpha=0.7, capsize=5,\n",
    "                  color='steelblue', edgecolor='black', error_kw={'linewidth': 2})\n",
    "    ax3.set_xticks(x_pos)\n",
    "    ax3.set_xticklabels(metrics_names, rotation=45, ha='right')\n",
    "    ax3.set_ylabel('Score', fontsize=12)\n",
    "    ax3.set_title('Media ¬± Desviaci√≥n Est√°ndar', fontweight='bold', fontsize=14)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    ax3.set_ylim(0, 1.05)\n",
    "    \n",
    "    for bar, mean, std in zip(bars, means, stds):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + std + 0.02,\n",
    "                f'{mean:.3f}¬±{std:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # 4. Tabla de estad√≠sticas\n",
    "    ax4 = plt.subplot(2, 2, 4)\n",
    "    ax4.axis('tight')\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    table_data = []\n",
    "    for metric_name, metric_key in zip(metrics_names, ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']):\n",
    "        row = [\n",
    "            metric_name,\n",
    "            f\"{stats[metric_key]['mean']:.4f}\",\n",
    "            f\"{stats[metric_key]['std']:.4f}\",\n",
    "            f\"{stats[metric_key]['min']:.4f}\",\n",
    "            f\"{stats[metric_key]['max']:.4f}\"\n",
    "        ]\n",
    "        table_data.append(row)\n",
    "    \n",
    "    table = ax4.table(cellText=table_data,\n",
    "                     colLabels=['Metric', 'Mean', 'Std', 'Min', 'Max'],\n",
    "                     cellLoc='center',\n",
    "                     loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(11)\n",
    "    table.scale(1, 2.5)\n",
    "    \n",
    "    for i in range(5):\n",
    "        table[(0, i)].set_facecolor('#40466e')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    ax4.set_title(f'Estad√≠sticas de {cv}-Fold Cross-Validation', \n",
    "                 fontweight='bold', fontsize=14, pad=20)\n",
    "    \n",
    "    plt.suptitle('Validaci√≥n Cruzada del Pipeline', fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESULTADOS DE VALIDACI√ìN CRUZADA:\")\n",
    "    print(\"=\"*80)\n",
    "    for metric_name, metric_key in zip(metrics_names, ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']):\n",
    "        print(f\"{metric_name:15s}: {stats[metric_key]['mean']:.4f} ¬± {stats[metric_key]['std']:.4f} \"\n",
    "              f\"(min={stats[metric_key]['min']:.4f}, max={stats[metric_key]['max']:.4f})\")\n",
    "    \n",
    "    return stats, fig\n",
    "\n",
    "# Validar pipeline completo\n",
    "cv_stats, fig_cv = cross_validate_pipeline(X_imb, y_imb, cv=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Resumen y Conclusiones\n",
    "\n",
    "## ‚úÖ Lo que hemos aprendido\n",
    "\n",
    "### 1. Limpieza de Datos\n",
    "* Los valores faltantes requieren an√°lisis cuidadoso (MCAR, MAR, MNAR)\n",
    "* KNN Imputation generalmente supera a m√©todos simples\n",
    "* Los outliers deben investigarse antes de eliminarlos\n",
    "* El escalamiento es crucial para muchos algoritmos\n",
    "\n",
    "### 2. Reducci√≥n de Dimensionalidad\n",
    "* **PCA**: R√°pido, interpretable, lineal\n",
    "  * √ötil para reducci√≥n real de dimensionalidad\n",
    "  * Preserva varianza global\n",
    "  \n",
    "* **t-SNE**: Lento, no interpretable, no lineal\n",
    "  * Excelente para visualizaci√≥n\n",
    "  * Preserva estructura local (clusters)\n",
    "\n",
    "### 3. Selecci√≥n de Atributos\n",
    "* **M√©todos Filter**: R√°pidos pero independientes del modelo\n",
    "* **M√©todos Wrapper**: M√°s lentos pero espec√≠ficos del modelo\n",
    "* **Consenso**: Combinar m√∫ltiples m√©todos aumenta robustez\n",
    "\n",
    "### 4. Balanceo de Clases\n",
    "* El desbalance severo sesga modelos hacia la mayor√≠a\n",
    "* **SMOTE** es generalmente superior a random oversampling\n",
    "* **ADASYN** adapta la s√≠ntesis a la densidad local\n",
    "* El balanceo debe aplicarse SOLO en training set\n",
    "\n",
    "### 5. Pipelines\n",
    "* Automatizan y estandarizan el preprocesamiento\n",
    "* Previenen data leakage\n",
    "* Facilitan reproducibilidad\n",
    "* Permiten comparaci√≥n justa de configuraciones\n",
    "\n",
    "## üéØ Mejores Pr√°cticas\n",
    "\n",
    "1. **Siempre dividir datos ANTES** de preprocesar\n",
    "2. **Documentar decisiones** de preprocesamiento\n",
    "3. **Validar el impacto** de cada transformaci√≥n\n",
    "4. **Usar validaci√≥n cruzada** para evaluar robustez\n",
    "5. **No eliminar datos sin investigar** primero\n",
    "6. **Balancear clases con cuidado** (solo en training)\n",
    "7. **Escalar antes de PCA** o m√©todos basados en distancia\n",
    "8. **Preferir pipelines** a c√≥digo ad-hoc\n",
    "\n",
    "## üìä Resultados Clave de Este Notebook\n",
    "\n",
    "De nuestros experimentos:\n",
    "* El escalamiento mejor√≥ m√©tricas en todos los casos\n",
    "* PCA redujo dimensiones sin perder desempe√±o significativo\n",
    "* SMOTE mejor√≥ recall de la clase minoritaria significativamente\n",
    "* El pipeline completo logr√≥ el mejor balance precision/recall\n",
    "\n",
    "## üöÄ Pr√≥ximos Pasos\n",
    "\n",
    "1. Aplicar estas t√©cnicas a sus propios datasets\n",
    "2. Experimentar con diferentes configuraciones\n",
    "3. Documentar el proceso de toma de decisiones\n",
    "4. Comparar m√∫ltiples estrategias sistem√°ticamente\n",
    "\n",
    "## üìö Referencias y Recursos\n",
    "\n",
    "* Scikit-learn Documentation: https://scikit-learn.org\n",
    "* Imbalanced-learn: https://imbalanced-learn.org\n",
    "* \"Feature Engineering and Selection\" - Kuhn & Johnson\n",
    "* \"Hands-On Machine Learning\" - Aur√©lien G√©ron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üí° Ejercicios Adicionales (Opcional)\n",
    "\n",
    "Pon a prueba tu comprensi√≥n:\n",
    "\n",
    "### Ejercicio 1: Dataset Personalizado\n",
    "Aplica el pipeline completo a uno de estos datasets:\n",
    "* Wine Quality\n",
    "* Iris\n",
    "* Digits\n",
    "* Tus propios datos\n",
    "\n",
    "### Ejercicio 2: Optimizaci√≥n de Hiperpar√°metros\n",
    "Usa GridSearchCV para optimizar:\n",
    "* N√∫mero de vecinos en KNN Imputer\n",
    "* N√∫mero de componentes en PCA\n",
    "* K en SMOTE\n",
    "\n",
    "### Ejercicio 3: An√°lisis de Sensibilidad\n",
    "Investiga c√≥mo var√≠a el desempe√±o al:\n",
    "* Cambiar porcentajes de valores faltantes\n",
    "* Variar grado de desbalance\n",
    "* Modificar cantidad de outliers\n",
    "\n",
    "### Ejercicio 4: Pipeline Avanzado\n",
    "Extiende el pipeline para:\n",
    "* Manejar variables categ√≥ricas\n",
    "* Incluir ingenier√≠a de features\n",
    "* Probar m√∫ltiples clasificadores\n",
    "\n",
    "---\n",
    "\n",
    "**¬°Excelente trabajo completando este m√≥dulo!** üéâ\n",
    "\n",
    "Has dominado las t√©cnicas fundamentales de limpieza y preparaci√≥n de datos que son esenciales para cualquier proyecto de ciencia de datos o machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
